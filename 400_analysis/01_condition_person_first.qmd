---
title: "Condition-first vs person-first language"
author: "Darya Vanichkina"
fig-cap-location: bottom
---

In this notebook, we explore whether there is a difference in the use of condition- vs person-first language in the Australian Obesity Corpus.

## Executive summary

**1.  Condition-first language is used in 9-14% of articles from all sources, while person-first language is used in less than 1% of articles.**

**2.  Condition-first language is used in 7-14% of articles per year across the study time period, while person-first language is used in 0.17-1.14% of articles per year.**

**3.  Person-first language is present in approximately the same number of articles in broadsheet and tabloid newspapers, whereas articles with only condition-first language are higher in number in tabloid publications.**

- The Pearson's Chi-squared test with Yates' continuity correction contrasting articles from tabloids and broadsheets that use only condition-first vs only person-first language indicate a significant link (X-squared = 4.8274, p-value = 0.02801) between type of publication and number of articles using a specific language type. The effect size is quite small (\<0.2), indicating that while the result is statistically significant, the fields are weakly associated. So while the number of articles from broadsheet and tabloids that use condition- and person-first language that we observe are different, the magnitude of this difference (i.e. the number of articles we see vs would expect by random chance) is not very high.
- The total number of uses of condition-first language we observe is higher in tabloids and lower in broadsheets than we would expect based on the word count in these subcorpora (p \< 0.001).
- The number of articles with condition-first language we observe is also higher in tabloids and lower in broadsheets than we would expect based on the total article count in these subcorpora (p \< 0.001).
- The total number of uses of person-first language we observe is somewhat higher in tabloids and lower in broadsheets than we would expect based on the word count in these subcorpora, but this result is not strongly significant (p \< 0.05).
- The number of articles with person-first language we observe is, in contrast, lower in tabloids and higher in broadsheets than we would expect based on the total article count in these subcorpora (p \< 0.002).

**4.  Person-first language is present in approximately the same number of articles in left- and right-leaning newspapers, whereas articles with only condition-first language are higher in number in right-leaning publications.**

- The total number of uses of condition-first language we observe is higher in right and lower in left-leaning publications than we would expect based on the word count in these subcorpora (p \< 0.001).
- The number of articles with condition-first language we observe is also is higher in right and lower in left-leaning publications than we would expect based on the total article count in these subcorpora (p \< 0.001).
- The total number of uses of person-first language we observe is higher in left and lower in right-leaning publications than we would expect based on the word count in these subcorpora (p \< 0.002).
- The number of articles with person-first language we observe is also is higher in left and lower in right-leaning publications than we would expect based on the total article count in these subcorpora (p \< 0.002).
- The Pearson's Chi-squared test with Yates' continuity correction contrasting articles from left- and right-leaning publications that use only condition-first vs only person-first language indicate a significant link (X-squared = 4.6405, p-value = 0.03123) between type of publication and number of articles using a specific language type. The effect size is, however, also negligible, indicating that while the result is statistically significant, the fields are weakly associated. This indicates that while we do see more articles that use only condition-first language and fewer that use person-first language in tabloids than in broadsheets, the difference in numbers between these observed number of articles and what we would expect by random chance is not very high.

**5.  Re-sampling the corpus 10000 times to select 1000 articles at a time without replacement results in a mean of 4 articles per 1000 using person-first language, and 122 per 1000 using condition-first language - so more articles in the corpus use condition-first language than person-first language.**

- The Welch Two Sample t-test testing the difference between person_first and condition_first bootstrapping (mean of person_first = 4.11, mean of condition_first = 122.57) suggests that the effect is negative, statistically significant, and large (difference = -118.46, 95% CI \[-118.67, -118.26\], t(10751.55) = -1138.14, p \< .001; Cohen's d = -16.10, 95% CI \[-16.31, -15.88\]).

**6.In texts that use either condition-first, person-first languages or both, the frequency of condition-first language is higher (mean 4 words per 1000) than person-first (mean 2.7 words per 1000).**

- The Welch Two Sample t-test testing the difference between condition_first_frequencies and person_first_frequencies (mean of x = 4.34, mean of y = 2.67) suggests that the effect is positive, statistically significant, and small (difference = 1.66, 95% CI \[1.16, 2.17\], t(131.59) = 6.49, p \< .001; Cohen's d = 0.44, 95% CI \[0.30, 0.58\])

**7. Relative to the Advertiser, the Age, Australian, Canberra Times, Courier Mail and Sydney Morning Herald had lower frequency of condition-first language.**

```{r setup}
#| warning: false
#| message: false
library(here)
library(dplyr)
library(ggplot2)
library(ggvenn)
library(readr)
library(tidyr)
library(knitr)
library(ggrepel)
library(report)
library(lme4)
library(optimx)
# set ggplot2 to use the minimal theme for all figures in the document
# unless explicitly specified otherwise
theme_set(theme_minimal())

source(here::here("400_analysis", "functions.R"))

condition_first <- read_cqpweb("aoc_all_condition_first.txt")
person_first <- read_cqpweb("aoc_all_person_first.txt")
metadata <- read_csv(here("100_data_raw", "corpus_cqpweb_metadata.csv"))

additional_source_metadata <- read_csv(here("100_data_raw", "addition_source_metadata.csv"))

metadata_full <- inner_join(metadata, additional_source_metadata)

condition_first_annotated <- inner_join(
  metadata_full, condition_first, by = c("article_id" = "text")) %>% 
  mutate(frequency = 10^3*no_hits_in_text/wordcount_total) 
person_first_annotated <- inner_join(
  metadata_full, person_first, by = c("article_id" = "text"))%>% 
  mutate(frequency = 10^3*no_hits_in_text/wordcount_total) 
corpus_articlecounts <- read_csv(here("100_data_raw", "articlecounts_full.csv"), col_names = TRUE, skip = 1) %>% filter(year != "source") %>% rename(source = year)

```

As discussed in the exploratory data analysis, we use the Python-generated word counts to count the frequency of occurrences per thousand words, as these do not include counts for punctuation symbols and hence do not distort counts for longer texts.

We group articles into tabloids and broadsheets, and by orientation, in the following manner:

```{r}
#| tbl-cap: Classification of sources into types and by orientation.
#| label: tbl-uno
metadata_full |>
  select(source, source_type, orientation) |>
  distinct() |>
  kable()
```

## Total number of articles with each of the language usages

First, we explore how many articles (absolute numbers and relative to the total number of articles in each source) use condition-first vs person-first language.


```{r HowManyPerJournal}
#| tbl-cap: Number and percentage (out of 100%) of articles in which person-first and condition-first language is used in the corpus, by publication.
#| label: tbl-2
condition_person_rbound <-
  rbind(
    articles_per_journal(person_first_annotated, "Person-first"),
    articles_per_journal(condition_first_annotated, "Condition-first"))

# generate how many articles per source are in the corpus
corpus_total_articles_bysource <-
  corpus_articlecounts %>% 
  rowwise() %>% 
  mutate(total = sum(c_across(where(is.numeric)))) %>% 
  select(source, total)


condition_person_rbound %>% 
  select(-year) %>% 
  group_by(type) %>% 
  count(source) %>%
  inner_join(corpus_total_articles_bysource) %>%
  mutate(percent = round(100*n/total, 2)) %>%
  rename(count = n) %>%
  pivot_wider(id_cols = source, names_from = type, values_from = c(count, total, percent), names_glue = "{type} {.value}") %>%
  rename(Total_articles = `Person-first total`) %>%
  select(-`Condition-first total`) %>%
  kable()
```

We can see that condition-first language is used in 9-14% of articles, while person-first language is used in less than 1% of articles across all sources.



```{r HowManyPerYear}
#| tbl-cap: Number and percentage (out of 100%) of articles in which person-first and condition-first language is used in the corpus, by year.
#| label: tbl-3


corpus_total_articles_byyear <-
  corpus_articlecounts %>%
  pivot_longer(cols = -source, names_to = "year", values_to = "number_of_articles" ) %>%
  select(-source) %>%
  group_by(year) %>%
  summarise(total = sum(number_of_articles)) %>%
  mutate(year = as.numeric(year))

# count the number of articles per source that use person first language
condition_person_rbound %>% 
  select(-source) %>% 
  group_by(type) %>% 
  count(year) %>%
  inner_join(corpus_total_articles_byyear) %>%
  mutate(percent = round(100*n/total, 2)) %>%
  rename(count = n) %>%
  pivot_wider(id_cols = year, names_from = type, values_from = c(count, total, percent), names_glue = "{type} {.value}") %>%
  rename(Total_articles = `Person-first total`) %>%
  select(-`Condition-first total`) %>%
  kable()
```

We can see that condition-first language is used in 7-14% of articles per year across the study time period, while person-first language is used in 0.17-1.14% of articles per year.

Furthermore, the numbers of articles that use person-first language within the corpus are quite small, so we cannot simultaneously explore whether this type of language changes across both publication and year:

```{r BothYearAndSource}
#| tbl-cap: Number of articles that use person-first language by source and year in the Australian Obesity Corpus.
#| label: tbl-4
assess_year_source(person_first_annotated) 
```

There is also not a lot of articles that use such language from each publication (2-23 articles, mean 9.9 +/- 7.14), so modelling the trend by publication is unlikely to result in meaningful data.

We do have a reasonable number of articles that use condition-first language, so we can model this if desired (except for the Brisbane Times and Daily Telegraph, where we are missing data prior to 2014):

```{r}
#| tbl-cap: Number of articles that use condition-first language by source and year in the Australian Obesity Corpus.
#| label: tbl-5
assess_year_source(condition_first_annotated) 
```

Also, among articles that use person-first language, nearly half will also use condition-first language in the same article:

```{r vennDiag}
#| label: fig-one
#| fig-cap: Number of articles that use person-first, condition-first or both language types within the same article.
tmpfig <-
  ggvenn(list(
  `Condition first` = condition_first_annotated$article_id, 
  `Person first` = person_first_annotated$article_id),
  fill_color = c("white", "white"))

ggsave(
  plot=tmpfig,
  device = "png",
       here::here("400_analysis","venn_diagram_condition_person_first.png"),
       bg = "white", 
       width = 4,
       height = 4)

ggvenn(list(
  `Condition first` = condition_first_annotated$article_id, 
  `Person first` = person_first_annotated$article_id),
  fill_color = c("#0073C2FF", "#CD534CFF"))
```

This means that comparing the use of person-first and condition-first language using a Chi-square test will not be appropriate, as the same article will be counted towards both condition-first and person-first language.

We can, however, compare the number of articles that use either language type (i.e. ONLY condition-first and only person-first) by type of publication:

```{r LangSourcetype}
#| tbl-cap: Number of articles that use either language type (i.e. ONLY condition-first and only person-first) by type of publication
#| label: tbl-7
language_sourcetype_table <-
  get_article_counts_nooverlap(condition_first_annotated,
                     person_first_annotated,
                     source_type)

language_sourcetype_table %>% 
  kable()
```

We can see that person-first language is present in approximately the same number of articles in broadsheet and tabloid newspapers, whereas articles with only condition-first language are higher in number in tabloid publications.

```{r chisq_LangSourcetype}
#| tbl-cap: Results of Chi-Square test of number of articles that use only person-first and only condition-first language in the corpus.
#| label: tbl-8
chisq_source_res <- chisq.test(language_sourcetype_table[,c("condition-first", "person-first")])

prettyrbind_chisq_result(
  df = language_sourcetype_table[,c("condition-first", "person-first")], 
  chisq_source_res, 
  prefix = "corrected") |> kable()

```

The Chi-square test results in a p-value that is less than 0.05, indicating a significant link between type of publication and type of language used.

The effect size is quite small (\<0.2), indicating that while the result is statistically significant, the fields are only weakly associated.

There has been criticism of use of the Yates correction, so we also provide the uncorrected results below:

```{r}
#| tbl-cap: Results of Chi-Square test without Yates continuity correction of number of articles that use only person-first and only condition-first language in the corpus.
#| label: tbl-9
chisq_source_res_uncorr <- chisq.test(language_sourcetype_table[,c("condition-first", "person-first")], correct = F)

prettyrbind_chisq_result(
  df = language_sourcetype_table[,c("condition-first", "person-first")], 
  chisq_source_res_uncorr, 
  prefix = "uncorrected") |> kable()
```

Let's next use a similar approach to identify whether left- or right- leaning publications are different in their use of condition- vs person-first language. What is the total number of articles that use EITHER condition-first or person-first language by orientation of publication?

```{r}
#| tbl-cap: Number of articles that use either condition-first or person-first language by orientation of publication.
#| label: tbl-10
language_orientation_table <- get_article_counts_nooverlap(condition_first_annotated, person_first_annotated, var = orientation)

language_orientation_table %>% kable()
```

Next, let's run a Chi-square test on this contingency table:

```{r}
#| tbl-cap: Results of Chi-Square test without Yates continuity correction of number of articles that use only person-first and only condition-first language in the corpus.
#| label: tbl-11
chisq_language_orientation_corr <- chisq.test(language_orientation_table[,c("condition-first", "person-first")])

prettyrbind_chisq_result(
  df = language_orientation_table[,c("condition-first", "person-first")], 
  chisq_language_orientation_corr, 
  prefix = "corrected") |> kable()
```

The Chi-square test of independence is significant.

However, the effect size is negligible (\<= 0.2), indicating that once again the fields are only weakly associated.

There has been criticism of use of the Yates correction, so we also provide the uncorrected results below:

```{r}
#| tbl-cap: Results of Chi-Square test without Yates continuity correction of number of articles that use only person-first and only condition-first language in the corpus.
#| label: tbl-12
chisq_language_orientation_uncorr <- chisq.test(language_orientation_table[,c("condition-first", "person-first")], correct = F)

prettyrbind_chisq_result(
  df = language_orientation_table[,c("condition-first", "person-first")], 
  chisq_language_orientation_uncorr, 
  prefix = "uncorrected") |> kable()
```

### Comparing article counts that use condition-first, person-first and no language type

As discussed above, the corpus contains articles that use condition-first, person-first and neither of these two language types. We can use repeated sampling of 1000 articles from the corpus 10000 times to explore how frequently we would observe articles from each of the three groups.

```{r resampleInsanity, cache=TRUE}
single_row <- function(x) {
  no_hits_cond_first <-
  condition_first_annotated %>% 
  filter(article_id %in% x) %>%
  summarise(count_condition_first = sum(no_hits_in_text)) %>% 
  pull()

no_hits_person_first <-
  person_first_annotated %>% 
  filter(article_id %in% x) %>%
  summarise(count_person_first = sum(no_hits_in_text)) %>% 
  pull()

total_words <-
  metadata_full %>%
  filter(article_id %in% x) %>%
  summarise(wc_total = sum(wordcount_total)) %>% 
  pull()

cbind(data.frame(
  condition_first = sum(x %in% condition_first_annotated$article_id),
  person_first = sum(x %in% person_first_annotated$article_id),
  freq_cond_first = 10^6*no_hits_cond_first/total_words,
  freq_pers_first = 10^6*no_hits_person_first/total_words
))
}






diff_boot <- purrr::map_dfr(
  1:10000,
  ~single_row(sample(metadata_full$article_id, 1000, replace = FALSE))
  )
```

We can visualise the observed counts per 1000 articles from the 10000 resamples:

```{r compareMeanResamples}
#| fig-cap: Histogram of observed counts of person-first and condition-first language from 10000 resamples  of 1000 articles each of the Australian Obesity Corpus.
#| label: fig-2
diff_boot %>%
  select(-starts_with("freq")) %>%
  pivot_longer(cols = everything(),
               names_to = "language_type", 
               values_to = "count_per_10000_articles") %>%
  ggplot(aes(x = count_per_10000_articles,
             fill = language_type)) +
  geom_histogram(bins = 150) + theme(legend.position = "bottom") + 
  labs(
    x = "Count per 1000 articles sampled",
    y = "Resamples with observed count",
    caption = "Total 10 000 resamples of corpus with 1000 articles each"
  )
```

We can compare the mean of these two observed resamples:

```{r results="asis"}
PersonFirst <- diff_boot$person_first
ConditionFirst <- diff_boot$condition_first
report(t.test(PersonFirst, ConditionFirst))
```

This shows that on average of every 1000 articles sampled from the corpus, 4 will use person-first and 122 will use condition-first language.

A non-parametric FP test supports this result:

```{r}
fp_test(
  wc1 = ConditionFirst,
  wc2 = PersonFirst,
  label1 = "condition",
  label2 = "person", 
  dist = mydistribution
)
ConditionFirst <- NULL
PersonFirst <- NULL
```

### Comparing the frequency of condition-first, person-first and no language type

We can also look at the frequenty of the different language used across the two sets of resamples.

We can visualise the observed frequency per million words from the 10000 resamples:

```{r compareMeanFreq}
#| fig-cap: Histogram of frequency per million words in subcorpus of person-first and condition-first language from 10000 resamples of 1000 articles each of the Australian Obesity Corpus.
#| label: fig-3
diff_boot %>%
  select(starts_with("freq")) %>%
  pivot_longer(cols = everything(),
               names_to = "language_type", 
               values_to = "freq_per_million_words") %>%
  ggplot(aes(x = freq_per_million_words,
             fill = language_type)) +
  geom_histogram(bins = 150) + theme(legend.position = "bottom") + 
  labs(
    x = "Frequency per million words in subcorpus",
    y = "Resamples with observed count",
    caption = "Total 10 000 resamples of corpus with 1000 articles each"
  )
```

We can compare the mean of these two observed resamples:

```{r results="asis"}
ConditionFirst <- diff_boot$freq_cond_first
PersonFirst <- diff_boot$freq_pers_first

report(t.test(PersonFirst, 
              ConditionFirst))
```

This shows that on average of every 1000 articles sampled from the corpus, 4 will use person-first and 122 will use condition-first language.

A non-parametric FP test confirms this result:

```{r}
fp_test(
  wc1 = ConditionFirst,
  wc2 = PersonFirst,
  label1 = "condition",
  label2 = "person", 
  dist = mydistribution
)
```

## Comparing the number of phrases that use condition-first vs person-first language

We can also take a different approach, comparing the number of phrases that use each language type; Here each phrase will contribute only to one group, i.e. be counted towards either person-first or condition-first. A phrase is defined in this context as an instance of language use, for example the article "AD150801123" contains 7 phrases, numbered below, that are classified by CQPweb as condition-first language:

> The discovery by the Murdoch Childrens Institute raises hope that if we can tackle **obesity in childhood(1)** we can avoid a tsunami of **obesity-related(2)** health expenses in the future.
>
> \<..\> "The findings will have major implications for how we treat **childhood obesity(3)**," says Professor Sabin.
>
> A review by the Murdoch Childrens Research Institute published in the Journal of Paediatrics and Child Health has found **childhood obesity(4)** has doubled in prevalence since the 1980s. Professor Sabin says while rates of **childhood overweight and obesity(5)** have plateaued the severity of the problem has increased.
>
> "**Childhood obesity(6)** has become a global crisis and is one of the world's most pressing public health issues," he says.The Murdoch Childrens Institute is undertaking a number of studies and programs to combat **childhood obesity(7)**.

We do, however, need to confirm that most articles have a small number of phrases, vs a small number of articles with a large number of phrases fully underpinning our counts. Let's look at how many articles have how many counts of each language usage:

```{r countPhrases}
#| tbl-cap: CQP-web determined number of hits in text, of condition-first and person-first language. 
#| label: tbl-16
count_language_types <- condition_first_annotated %>%
  dplyr::select(article_id, no_hits_in_text) %>%
  mutate(type="condition-first") %>%
  rbind({
    person_first_annotated %>%
      select(article_id, no_hits_in_text) %>%
      mutate(type="person-first")
  })

count_language_types %>% 
  group_by(no_hits_in_text, type) %>%
  count() %>%
  pivot_wider(names_from=type, values_from = n, values_fill = 0) %>%
  kable()
  
```

```{r}
#| fig-cap: Histogram of CQP-web determined number of hits in text, of condition-first and person-first language. 
#| label: fig-l17
count_language_types %>%
  ggplot(aes(x = no_hits_in_text)) + geom_bar() + facet_grid(type~., scales = "free_y")
```


Most articles have 1-2 uses of person-first/condition-first language, but some have up to 13 uses. How many total instances are there?

```{r chisq_table}
#| tbl-cap: Total number of CQP-web determined instances and articles with at least one instance of condition-first and person-first language. 
#| label: tbl-l18
count_language_types %>% 
  group_by(type) %>% 
  summarise( instances = sum(no_hits_in_text),
             articles = n()) %>%
  kable()
  
```

## Relative frequency of condition- vs person-first language

Let's explore what the relative frequency, calculated as `10^3*no_hits_in_text/wordcount_total` (where `no_hits_in_text` is determined by CQPweb and `wordcount_total` is the Python word count), of condition-first vs person-first language looks like.

```{r plot}
#| fig-cap: Histogram of relative frequency per 1000 words of condition-first and person-first language in the Australian Obesity Corpus.
#| label: fig-l19
freq_1 <- condition_first_annotated %>%
  select(frequency) %>%
  mutate(condition = "Condition-first language") %>%
  rbind({
  person_first_annotated %>%
  select(frequency) %>%
  mutate(condition = "Person-first language")
  }) 

freq_1_gt100words <- condition_first_annotated %>%
  filter(wordcount_from_metatata >= 100) %>%
  select(frequency) %>%
  mutate(condition = "Condition-first language") %>%
  rbind({
  person_first_annotated %>%
  filter(wordcount_from_metatata >= 100) %>%
  select(frequency) %>%
  mutate(condition = "Person-first language")
  }) 

freq_1 %>%
  ggplot(aes(x = frequency, fill = condition)) +
  facet_grid(condition~., scales = "free_y") +
  geom_histogram(bins = 100) + 
  xlab("Frequency per thousand words") + 
  ylab("Number of articles") + theme(legend.position = "none") +
  geom_vline(xintercept = 20, lty=2)

```

Let's create a box plot to compare the frequency per thousand words:

```{r freq_boxplot}
#| fig-cap: Box plot comparing the distribution of the relative frequency per 1000 words of condition-first and person-first language in the Australian Obesity Corpus.
#| label: fig-l20
freq_1 %>%
  ggplot(aes( y =  frequency, x = condition)) + 
  geom_boxplot(outlier.shape = NA) +
  scale_y_continuous(limits = quantile(freq_1$frequency, c(0.05, 0.95))) +
  labs(
    x = "",
    y = "Frequency per thousand words"
  )

```

We can then use a two-sample t-test to compare the mean frequency of condition-first vs person-first language in the corpus:

```{r ttest, results="asis"}
condition_first_frequencies <- freq_1 %>% 
  filter(condition == "Condition-first language") %>% 
  pull(frequency)

person_first_frequencies <- freq_1 %>%
  filter(condition == "Person-first language") %>% 
  pull(frequency)



report(t.test(condition_first_frequencies,
       person_first_frequencies
       ))
```

In texts that use either condition-first, person-first languages or both, the frequency of condition-first language is higher (mean 4 words per 1000) than person-first (mean 2.7 words per 1000).

We can also use a non-parametric FP test to support this:

```{r}
fp_test(
  wc1 = condition_first_frequencies,
  wc2 = person_first_frequencies,
  label1 = "condition",
  label2 = "person", 
  dist = mydistribution
)
```

The below plot shows the article ids of articles with a word count less than 100 for person-first language, and article ids with word counts less than 100 where the frequency is greater than 20 for condition-first language

```{r lowWCtextsPlot}
#| fig-cap: Article ids of articles with (top) word counts less than 100 where the frequency is greater than 20 for condition-first language, and (bottom) a word count less than 100 for person-first language.
#| label: fig-l21
condition_first_annotated %>%
  # select only texts less than 100 words
  filter(wordcount_total <= 100) %>%
  select(article_id, frequency) %>%
  mutate(condition = "Condition-first language") %>%
  # note that for condition-first only looking at those that are very high frequency here
  filter(frequency >= 20) %>%
  rbind({
  person_first_annotated %>%
  # select only texts less than 100 words
  filter(wordcount_total <= 100) %>%
  select(article_id, frequency) %>%
  mutate(condition = "Person-first language")
  }) %>% 
  group_by(frequency) %>%
  mutate(cnt = n()) %>%
  ggplot(aes(x = frequency, y = cnt, fill = condition, label = article_id)) +
  facet_grid(condition~., scales = "free_y") +
  geom_text_repel(check_overlap = TRUE, angle = 90) +
  xlab("Frequency per thousand words") + 
  ylab("Article ID & count") + theme(legend.position = "none") +
  geom_vline(xintercept = 20, lty=2)
```

There are a few texts with very high frequencies. These mostly occur in cases where the text length itself is quite short. We can consider whether we want to filter out texts with a word count of less than 100 words.

If we run a t-test on the dataset filtered to only contain texts greater than 100 words, we can see that while the results are still significant, the mean difference is less.

```{r ttest2, results="asis"}
condition_first_frequencies_gt100 <- freq_1_gt100words %>% 
  filter(condition == "Condition-first language") %>% 
  pull(frequency)

person_first_frequencies_gt100 <- freq_1_gt100words %>%
  filter(condition == "Person-first language") %>% 
  pull(frequency)

report(t.test(
  condition_first_frequencies_gt100,
  person_first_frequencies_gt100
       ))
```

This is supported by a non-parametric FP test:

```{r}
fp_test(
  wc1 = condition_first_frequencies_gt100,
  wc2 = person_first_frequencies_gt100,
  label1 = "condition",
  label2 = "person", 
  dist = mydistribution
)
```

## Person-first language frequency

Let's visualise the frequency, calculated as `10^3*no_hits_in_text/wordcount_total` (where `no_hits_in_text` is determined by CQPweb and `wordcount_total` is the Python word count), of person-first language by publication:

```{r vizPersonPub}
#| label: fig-l22
#| fig-cap: Box and jitter plot showing the summary statistics and raw values of the frequency per 1000 words of person-first language in the different sources, with boxes coloured based on source type. This shows, for example, that the Northern Territorian has the highest median frequncy of person-first language, but this is because the summary statistics are only based on two quite divergent data points.

person_first_annotated %>%
  select(source, frequency, year, source_type) %>%
  ggplot(aes(x = reorder(source, frequency), 
             y = frequency, 
             fill = source_type)) +
  geom_boxplot(outlier.shape = NA) + 
  theme(axis.text.x=element_text(angle = 45, hjust =1),
        legend.position = "bottom") +
  labs(x = NULL, 
       y = "Frequency per thousand words") +
  geom_jitter(width = 0.25, alpha = 0.5) 

```

And per year:

```{r}
#| label: fig-l23
#| fig-cap: Box and jitter plot showing the summary statistics and raw values of the frequency per 1000 words of person-first language by year and source type. This shows that due to the low numbers of articles using person-first language, there is substantial variability in the summary statistics (median, IQR) across the years.

person_first_annotated %>%
  select(source, frequency, year, source_type) %>%
  ggplot(aes(x = as.factor(year), y = frequency, 
             fill = source_type,  shape = source_type)) +
  facet_wrap(~source_type) +
  geom_boxplot(outlier.shape = NA) + theme_bw() +
  theme(axis.text.x=element_text(angle = 45, hjust =1),
        legend.position = "bottom") +
  labs(x = NULL, y = "Frequency per thousand words") +
  geom_jitter(width = 0.1, alpha = 0.5) 
```

## Condition- and person-first language normalised by total article length

We can also look at the frequency of person-first and condition-first language by dividing the number of observations of each language type by the total word count of articles in which they are found (i.e. frequency per 1000 words not on a per-article basis, but on a per total word count of articles in the group).

For example, in the table below, there are 6 instances of person-first language in 2008, and the total word count of the articles that contain these 6 instances is 4386 words. Therefor, the normalised frequency is `6*1000/4386 = 1.37`.

```{r freqPerSubcorpus}
#| label: tbl-l24
#| tbl-cap: Number of instances of the two language types by year, total word count of articles that use the specified language type and the relative frequency of instances per 1000 words across all articles that year.

freq_per_subcorpus <- function(df, group_vars){
  df %>%
  group_by(!!!group_vars) %>%
  summarise(
    total_instances = sum(no_hits_in_text),
    total_wc = sum(wordcount_total),
    instances_per_1000 = 1000 * total_instances/total_wc
  )
}

inner_join(
  {freq_per_subcorpus(person_first_annotated, 
                      group_vars=vars(year)) %>%
      rename("instances_person_first" = "total_instances",
             "wc_personfirst" = "total_wc",
             "person first instances per 1000 words" = "instances_per_1000")
  },
  {freq_per_subcorpus(
    condition_first_annotated, 
    group_vars=vars(year)) %>%
      rename("instances_cond_first" = "total_instances",
             "wc_condfirst" = "total_wc",
             "condition first instances per 1000 words" = "instances_per_1000")
  }) %>% kable(digits = 2)

```

```{r}
#| label: fig-l25
#| fig-cap: Visualisation of the above relative frequency per 1000 words of person-first and condition-first language by year.

rbind({freq_per_subcorpus(person_first_annotated, 
                   group_vars=vars(year)) %>%
    mutate(language = "Person-first")},
      {freq_per_subcorpus(condition_first_annotated, 
                   group_vars=vars(year)) %>%
    mutate(language = "Condition-first")}
        ) %>%
  ggplot(aes(x = as.factor(year),
             y = instances_per_1000,
             col = language)) +
  geom_point() +
  labs(x = "Year",
       y = "Frequency per 1000 words",
       caption = "Instances per 1000 words across all articles that use language type")
```



We can also look at this across sources:

```{r}
#| label: tbl-l26
#| tbl-cap: Number of instances of the two language types by source, total word count of articles that use the specified language type and the relative frequency of instances per 1000 words across all articles from that source.

inner_join(
  {freq_per_subcorpus(person_first_annotated, 
                      group_vars=vars(source)) %>%
      rename("instances_person_first" = "total_instances",
             "wc_personfirst" = "total_wc",
             "person first instances per 1000 words" = "instances_per_1000")
  },
  {freq_per_subcorpus(
    condition_first_annotated, 
    group_vars=vars(source)) %>%
      rename("instances_cond_first" = "total_instances",
             "wc_condfirst" = "total_wc",
             "condition first instances per 1000 words" = "instances_per_1000")
  }) %>% kable(digits = 2)

```

```{r}
#| label: fig-l28
#| fig-cap: Visualisation of the above relative frequency per 1000 words of person-first and condition-first language by source.
rbind({freq_per_subcorpus(person_first_annotated, 
                   group_vars=vars(source)) %>%
    mutate(language = "Person-first")},
      {freq_per_subcorpus(condition_first_annotated, 
                   group_vars=vars(source)) %>%
    mutate(language = "Condition-first")}
        ) %>%
  ggplot(aes(x = source,
             y = instances_per_1000,
             col = language)) +
  geom_point() +
  labs(x = "Source",
       y = "Frequency per 1000 words",
       caption = "Instances per 1000 words across all sources that use language type") +
  theme(axis.text.x = element_text(angle=90))
```

We can also do this across source and year:

```{r}
#| label: tbl-l29
#| tbl-cap: Number of instances of the two language types by source, year, total word count of articles that use the specified language type and the relative frequency of instances per 1000 words across all articles from that source that year.

inner_join(
  {freq_per_subcorpus(person_first_annotated, 
                      group_vars=vars(source, year)) %>%
      rename("instances_person_first" = "total_instances",
             "wc_personfirst" = "total_wc",
             "person first instances per 1000 words" = "instances_per_1000")
  },
  {freq_per_subcorpus(
    condition_first_annotated, 
    group_vars=vars(source, year)) %>%
      rename("instances_cond_first" = "total_instances",
             "wc_condfirst" = "total_wc",
             "condition first instances per 1000 words" = "instances_per_1000")
  }) %>% kable(digits = 2)

```


```{r}
#| label: fig-l30
#| fig-cap: Visualisation of the above relative frequency per 1000 words of person-first and condition-first language by source and year.
rbind({freq_per_subcorpus(person_first_annotated, 
                   group_vars=vars(source, year)) %>%
    mutate(language = "Person-first")},
      {freq_per_subcorpus(condition_first_annotated, 
                   group_vars=vars(source, year)) %>%
    mutate(language = "Condition-first")}
        ) %>%
  ggplot(aes(x = as.factor(year),
             y = instances_per_1000,
             col = language)) +
  geom_jitter() +
  facet_wrap(~source) +
  labs(x = "Year",
       y = "Frequency per 1000 words",
       caption = "Instances per 1000 words across all sources & years that use language type") +
  theme(axis.text.x = element_text(angle=90))
```


Note that above we considered the word count ONLY in articles that featured that particular language type in the denominator 

Next, we conduct the same analysis, but including all articles from that particular source, year or both (irrespective of whether they feature the language type).

```{r}
#| label: tbl-l31
#| tbl-cap: Number of instances of the two language types by year, total word count of articles published that year and the relative frequency of instances per 1000 words across all articles from that source.

freq_entire_corpus <- function(df, group_vars){
  df %>%
  group_by(!!!group_vars) %>%
  summarise(
    person_first_instances = sum(person_first),
    condition_first_instances = sum(condition_first),
    total_wc = sum(wordcount_total),
    person_first_per_1000 = 1000 * person_first_instances/total_wc,
    cond_first_per_1000 = 1000 * condition_first_instances/total_wc
  )
}

condition_person_comparison_together <- 
  full_join(
    full_join(
      {person_first_annotated %>%
          select(article_id, no_hits_in_text) %>%
          rename(person_first = no_hits_in_text)},
      {condition_first_annotated %>%
          select(article_id, no_hits_in_text) %>%
          rename(condition_first = no_hits_in_text)}
    ),
    {metadata_full %>%
        select(article_id, source, year, wordcount_total)}) %>%
  # fill NAs with 0
  mutate_if(is.numeric,coalesce,0) 
  # now generate the instances
  
freq_entire_corpus(condition_person_comparison_together, vars(year)) %>% 
  kable()


```


```{r}
#| label: fig-l32
#| fig-cap: Visualisation of the above relative frequency per 1000 words of person-first and condition-first language by year, considering the word count of all articles in the corpus that year.

freq_entire_corpus(condition_person_comparison_together, vars(year)) %>%
  select(year, ends_with("per_1000")) %>%
  pivot_longer(cols = ends_with("1000"), names_to = "type", values_to = "value") %>%
  mutate(type = stringr::str_replace_all(type, "_per_1000", ""),
         type = case_when(type == "cond_first" ~ "Condition-first", TRUE ~ "Person-first")) %>%
  ggplot(aes(x = as.factor(year), y= value, col = type)) +
  geom_point() + 
  labs(x = "Year",
       y = "Instances per 1000 words in corpus by year",
       col = "")
```


We can see that if we consider the entire corpus, instances of condition-first language seem to be somewhat lower by year in 2017 onward. 

Let's look at the numbers by source, across all years:

```{r}
#| label: tbl-l33
#| tbl-cap: Number of instances of the two language types by source, total word count of articles published in that source in the corpus and the relative frequency of instances per 1000 words across all articles from that source.

freq_entire_corpus({condition_person_comparison_together %>% filter(!(source %in% c("Telegraph", "BrisTimes")))}, vars(source)) %>% 
  kable()


```

```{r}
#| label: fig-l34
#| fig-cap: Visualisation of the above relative frequency per 1000 words of person-first and condition-first language by source, considering the word count of all articles in the corpus from that source.

freq_entire_corpus({
  condition_person_comparison_together %>% 
    filter(!(source %in% c("Telegraph", "BrisTimes")))
}, vars(source)) %>%
  select(source, ends_with("per_1000")) %>%
  pivot_longer(cols = ends_with("1000"), names_to = "type", values_to = "value") %>%
  mutate(type = stringr::str_replace_all(type, "_per_1000", ""),
         type = case_when(type == "cond_first" ~ "Condition-first", TRUE ~ "Person-first")) %>%
  ggplot(aes(x = source, y= value, col = type)) +
  geom_point() + 
  labs(x = "Source",
       y = "Instances per 1000 words in corpus by source",
       col = "") +
  theme(axis.text.x = element_text(angle=90))
```


We can see that usage of condition-first language is quite varied by source.

Let's look at source and year simultaneously:

```{r}
#| label: tbl-l35
#| tbl-cap: Relative frequency per 1000 words of person-first and condition-first language by source and year, considering the word count of all articles in the corpus from that source in that year.

freq_entire_corpus({condition_person_comparison_together %>% filter(!(source %in% c("Telegraph", "BrisTimes")))}, vars(source, year)) %>% 
  kable()
```

```{r}
#| label: fig-l36
#| fig-cap: Visualisation of the above relative frequency per 1000 words of person-first and condition-first language by source and year, considering the word count of all articles in the corpus from that source in that year.

freq_entire_corpus({
  condition_person_comparison_together %>% filter(!(source %in% c("Telegraph", "BrisTimes")))
}, vars(source, year)) %>%
  select(source, year, ends_with("per_1000")) %>%
  pivot_longer(cols = ends_with("1000"), names_to = "type", values_to = "value") %>%
  mutate(type = stringr::str_replace_all(type, "_per_1000", ""),
         type = case_when(type == "cond_first" ~ "Condition-first", TRUE ~ "Person-first")) %>%
  ggplot(aes(x = as.factor(year), y= value, col = type)) +
  geom_point() + 
  facet_wrap(~source) +
  labs(x = "Year",
       y = "Instances per 1000 words in corpus by source & year",
       col = "") +
  theme(axis.text.x = element_text(angle=90))
```


## Condition-first language use

We can investigate the prevalence of condition-first language using goodness of fit tests, comparing their distribution in:

-   tabloids vs broadsheets
-   left and right leaning publications

We can do this by looking at:

-   the total number of instances in the subcorpus
-   the number of articles that feature this language type

### Tabloids vs broadsheets

The total number of uses of condition-first language we observe is higher in tabloids and lower in broadsheets than we would expect based on the word count in these subcorpora (p \< 0.001).

```{r}
#| label: tbl-l37
#| tbl-cap: Results of goodness of fit test of the number of uses of condition-first language we observe in tabloids and broadsheets relative to the total word count across these two types of sources.

chisq_instances_wc_normalised(condition_first_annotated, metadata_full, source_type)
```

The number of articles with condition-first language we observe is also higher in tabloids and lower in broadsheets than we would expect based on the total article count in these subcorpora (p \< 0.001).

```{r}
#| label: tbl-l38
#| tbl-cap: Results of goodness of fit test of the number of articles that use condition-first language we observe in tabloids and broadsheets relative to the total word count across these two types of sources.

chisq_articles_totalart_normalised(condition_first_annotated, metadata_full, source_type)
```

### Left vs right-leaning publications

The total number of uses of condition-first language we observe is higher in right and lower in left-leaning publications than we would expect based on the word count in these subcorpora (p \< 0.001).

```{r}
#| label: tbl-l39
#| tbl-cap: Results of goodness of fit test of the number of uses of condition-first language we observe in right and left leaning publications relative to the total word count across these two types.

chisq_instances_wc_normalised(condition_first_annotated, metadata_full, orientation)
```

The number of articles with condition-first language we observe is also is higher in right and lower in left-leaning publications than we would expect based on the total article count in these subcorpora (p \< 0.001).

```{r}
#| label: tbl-l40
#| tbl-cap: Results of goodness of fit test of the number of articles that use condition-first language we observe in left and right leaning publications relative to the total word count across these two types.

chisq_articles_totalart_normalised(condition_first_annotated, metadata_full, orientation)
```

## Person-first language use

### Tabloids vs broadsheets

The total number of uses of person-first language we observe is somewhat higher in tabloids and lower in broadsheets than we would expect based on the word count in these subcorpora, but this result is not strongly significant (p \< 0.05).

```{r}
#| label: tbl-l41
#| tbl-cap: Results of goodness of fit test of the number of uses of person-first language we observe in tabloids and broadsheets relative to the total word count across these two types of sources.

chisq_instances_wc_normalised(person_first_annotated, metadata_full, source_type)
```

The number of articles with person-first language we observe is, in contrast, lower in tabloids and higher in broadsheets than we would expect based on the total article count in these subcorpora (p \< 0.002).

```{r}
#| label: tbl-l42
#| tbl-cap: Results of goodness of fit test of the number of articles that use person-first language we observe in tabloids and broadsheets relative to the total word count across these two types of sources.

chisq_articles_totalart_normalised(person_first_annotated, metadata_full, source_type)
```

### Left vs right-leaning publications

The total number of uses of person-first language we observe is higher in left and lower in right-leaning publications than we would expect based on the word count in these subcorpora (p \< 0.002).

```{r}
#| label: tbl-l43
#| tbl-cap: Results of goodness of fit test of the number of uses of person-first language we observe in right and left leaning publications relative to the total word count across these two types.

chisq_instances_wc_normalised(person_first_annotated, metadata_full, orientation)
```

The number of articles with person-first language we observe is also is higher in left and lower in right-leaning publications than we would expect based on the total article count in these subcorpora (p \< 0.002).

```{r}
#| label: tbl-l44
#| tbl-cap: Results of goodness of fit test of the number of articles that use person-first language we observe in left and right leaning publications relative to the total word count across these two types.

chisq_articles_totalart_normalised(person_first_annotated, metadata_full, orientation)
```

## Condition-first language across time

As we discussed, we have sufficient data to explore the use of condition-first language across time and by type of publication, except for the Brisbane Times and Daily Telegraph, for which we are missing data from 2008-2013:

```{r condition-firstSummary}
#| label: tbl-l45
#| tbl-cap: Number of articles that use condition-first language by year and source.

assess_year_source(condition_first_annotated)

condition_first_annotated_for_modelling <- condition_first_annotated %>%
  filter( !(source %in% c("BrisTimes", "Telegraph")))
```

Let's look at the number of articles by publication and year. We can see that this number is declining; however, this is likely to be attributable to the overall decline in the number of articles featuring obes\*, as discussed in the exploratory data analysis section

```{r condFirstyearPub}
#| label: fig-l46
#| fig-cap: Line plot of the number of articles that use condition-first language by year and source.

condition_first_annotated %>%
  select(year, source, source_type) %>%
  group_by(year, source) %>%
  count() %>%
  ggplot(aes(x = year, y = n, col = source)) + 
  geom_line() +
  labs(x = "", y = "Number of articles") +
  theme(axis.text.x = element_text(angle = 90),
        legend.position = "NA") +
  scale_x_continuous(breaks = unique(condition_first_annotated$year)) + 
  facet_wrap(~source)
```

#### Normalised frequency

The normalised frequency is distributed log-normally across all texts:

```{r normDist}
#| label: fig-l47
#| fig-cap: Histogram of the natural logarithm of the normalised frequency of condition-first language across all articles.

condition_first_annotated %>%
  select(frequency) %>%
  ggplot(aes(x = log(frequency))) + 
  geom_histogram(bins = 75)
```

Let's look at the difference in frequency across time (only the variability of which should be sensitive to the number of articles per year, not the absolute values):

We can start by using a jitter plot:

```{r yearSourceBoxplot}
#| label: fig-l48
#| fig-cap: Jitter plot of raw values and loess smoothing (blue line) of natural logarithm of normalised frequency of condition-first language by year for each source. 

condition_first_annotated %>%
  ggplot(aes(x = as.factor(year), 
             y = log(frequency), 
             fill = year)) + 
  geom_jitter(alpha = 0.2) +
  geom_smooth(aes(group = source), col = "blue", method = "loess") +
  geom_hline(yintercept = 1, col = "red", lty = 3) + 
  facet_wrap(~source) + 
  theme(axis.text.x = 
          element_text(angle = 90, vjust = 0.5, hjust=1),
        legend.position = "NA") + 
  labs(
    x = "Year",
    y = "log(frequency per 1000 words)"
  )
```

Note that the dashed red line is always at the same position (with a value of `exp(1) = 2.72`). Comparing it with the blue line of best fit for each source for which we have complete data suggests that visually we cannot discern strong trends in the use of condition-first language across the study time period, so using variability-based neighbor clustering (VNC) is unlikely to provide meaningful results for this research question.

We can see that the Advertiser seems to have higher median frequencies than others, as does the Northern Territorian. Let's look at it grouped as tabloid vs broadsheet (with outliers not shown):

```{r condFirstYearPubNormTabBroadNoOut}
#| label: fig-l49
#| fig-cap: Boxplot of the frequency of condition-first language by year across tabloids and broadsheets, with outliers not shown.

condition_first_annotated %>%
  select(year, source, source_type, frequency) %>%
  mutate(year = as.factor(year)) %>%
  group_by(year, source_type) %>%
  ggplot(aes(x = year, y = frequency, fill = source_type)) +
  geom_boxplot(outlier.shape = NA) +
  coord_cartesian(ylim = quantile(condition_first_annotated$frequency, c(0.05, 0.95))) +
  labs(
    x = "",
    y = "Frequency per thousand words",
    fill = "Source type"
  )
```

It appears that median frequency in tabloids is somewhat higher, although the intervals do overlap across all years.

```{r condFirstYearPubNormTabBroadNoOut2}
#| label: fig-l50
#| fig-cap: Jitter plot of natural logarithm of frequency of condition first language per 1000 words by year, split across tabloids and broadsheets, with blue line showing linear trend.

condition_first_annotated %>%
  ggplot(aes(x = as.factor(year), 
             y = log(frequency), 
             fill = year)) + 
  geom_jitter(alpha = 0.2) +
  geom_smooth(aes(group = source_type), col = "blue", method = "lm") +
  geom_hline(yintercept = 1, col = "red", lty = 3) + 
  facet_wrap(~source_type) + 
  theme(axis.text.x = 
          element_text(angle = 90, vjust = 0.5, hjust=1),
        legend.position = "NA") + 
  labs(
    x = "Year",
    y = "log(frequency per 1000 words)"
  )
```

We can see that the frequency seems to decrease in broadsheets but not in tabloids across years.

Let's quickly look at differences by month:

```{r monthCondFirst}
#| label: fig-l51
#| fig-cap: Violin plot of frequency of condition-first language by month.

condition_first_annotated_for_modelling %>% 
  select(month_metadata, source, frequency) %>%
  ggplot(aes(y = frequency, x = month_metadata)) + 
  geom_violin()
```

The frequency doesn't seem to be different month to month, when visualised using a violin or box plots.

```{r noArtCondFirst}
#| label: fig-l52
#| fig-cap: Boxplot of frequency of condition-first language by month.

condition_first_annotated_for_modelling %>% 
  select(month_metadata, source) %>%
  group_by(month_metadata, source) %>%
  mutate(count_source = n()) %>%
  distinct() %>%
  ungroup() %>%
  ggplot(aes(y = count_source, x = month_metadata)) +
  geom_boxplot()
```

## Condition-first language - modelling frequency

We will use a linear mixed effects model to consider whether there are differences in the frequency of condition-first language use in broadsheets and tabloids across years, including whether there are differences in specific publications. We will also use simple linear models to explore

When constructing the model we will:

-   Use `log(frequency)` as the dependent variable, as this is normally distributed
-   Center and scale the date

```{r scale&setupbase}
condition_first_annotated_for_modelling$scaled_year <- scale(condition_first_annotated_for_modelling$year, scale = F)

library(broom.mixed)
# base model
m_0_base <- glm(log(frequency) ~ 1, family = gaussian, 
                data = condition_first_annotated_for_modelling)
# with year
m_0_year <- glm(log(frequency) ~ scaled_year, family = gaussian, 
                data = condition_first_annotated_for_modelling)

# with year and source type
m_0_yearsourcetype <- glm(log(frequency) ~ scaled_year + source_type, family = gaussian, 
                data = condition_first_annotated_for_modelling)

# with year and source type
m_0_yearsource <- glm(log(frequency) ~ scaled_year + source, family = gaussian, 
                data = condition_first_annotated_for_modelling)

# with source
m_0_source = lmer(log(frequency) ~ 1 + (1|source), REML = T, 
                  data = condition_first_annotated_for_modelling)

```

Does including a random intercept for each source improve our model?

```{r compareModels1}
#| label: tbl-l53
#| tbl-cap: Log-likelihood, Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), residual number of degrees of freedom and number of observations used to fit a range of general linear models.

rbind(
  {glance(m_0_base) %>% mutate(model = "Base") %>% dplyr::select(-df.null, -null.deviance, -deviance)},
  {glance(m_0_year) %>% mutate(model = "With year") %>% dplyr::select(-df.null, -null.deviance, -deviance)},
  {glance(m_0_yearsourcetype) %>% mutate(model = "With year & source type") %>% dplyr::select(-df.null, -null.deviance, -deviance)},
  {glance(m_0_yearsource) %>% mutate(model = "With year & source") %>% dplyr::select(-df.null, -null.deviance, -deviance)},
  {glance(m_0_source) %>% mutate(model = "With source") %>% dplyr::select(-sigma, -REMLcrit)}
) %>% 
  arrange(AIC) %>%
  kable()
```

Yes, it seems that the AIC and BIC are reduced while the logLik is higher for the model that includes source and year. So, yes, it seems using a random effects model for source may be an option.

Now let's build several different random effects models:

-   Including year as a fixed effect
-   Including each specific source (random effect) individually and year

```{r lmeModelsGenerate}
#library(afex)


m_1_base <- lmer(log(frequency) ~ 1 + (1|source), 
                 data = condition_first_annotated_for_modelling,
                 REML = FALSE, 
                 control = lmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))
# random intercept for each source
m_1_year <- lmer(log(frequency) ~ scaled_year + (1|source), 
                 data = condition_first_annotated_for_modelling,
                 REML = FALSE, 
                 control = lmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))
# random intercept for each source
m_1_year_sourcetype <- lmer(log(frequency) ~ scaled_year + source_type +(1|source), 
                 data = condition_first_annotated_for_modelling,
                 REML = FALSE, 
                 control = lmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))
# random slope and intercept for each source
m_1_yearsource <- lmer(log(frequency) ~ scaled_year + (scaled_year|source), 
                 data = condition_first_annotated_for_modelling,
                 REML = FALSE, 
                 control = lmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))

# random slope and intercept for each source
m_1_full <- lmer(log(frequency) ~ scaled_year + source_type + (scaled_year|source), 
                 data = condition_first_annotated_for_modelling,
                 REML = FALSE, 
                 control = lmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))

# random intercept for each source type
m_1_year_sourcetype_nosource <- 
  lmer(log(frequency) ~ scaled_year +(1|source_type), 
                 data = condition_first_annotated_for_modelling,
                 REML = FALSE, 
                 control = lmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))


# use the all_fit function to assess which optimisers work
#all_fit(m_1_yearsource)

# m_1_yearsource_apex <- 
#   mixed(log(frequency) ~ scaled_year + (scaled_year|source), 
#       data = condition_first_annotated_for_modelling,
#       method = "PB",
#       REML=FALSE,
#       control = lmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))
# m_1_yearsource_apex
```

We end up needing to use the `nlminb` optimiser from the `optimx` library (originally used by lme4), as the default REML fails to converge for the most complex model.

```{r}
#| label: tbl-l54
#| tbl-cap: Log-likelihood, Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), residual number of degrees of freedom and number of observations used to fit a range of random effects models.

purrr::map_dfr(list(
  m_1_base,
  m_1_year,
  m_1_year_sourcetype,
  m_1_yearsource,
  m_1_year_sourcetype_nosource,
  m_1_full),
        ~(glance(.x))) %>%
  mutate(model = c(
    "1 + (1|source)",
    "scaled_year + (1|source)",
    "scaled_year + source_type +(1|source)",
    "scaled_year + (scaled_year|source)",
    "scaled_year + (1|sourcetype)",
    "scaled_year + source_type + (scaled_year|source)"
  )) %>%
  arrange(AIC) %>%
  kable()
```

The full model (scaled_year + source_type + (scaled_year\|source)) has the lowest AIC and highest log-Likelihood among the mixed effects models. However, it's AIC is not that different (7524 vs 7525) to the simpler model `scaled_year + source`, while the simpler model has a lower BIC and higher logLik.

Let's compare the two models: the full mixed effects model and the simple `scaled_year + source`

```{r compModels}
anova(m_1_full, m_0_yearsource)
```

It seems that the more complex model does not offer a substantial improvement in fit over the simpler one. Let's summarise that model.

```{r  results="asis"}
report::report(m_0_yearsource)
```

If we use the BIC as our model selection criteria instead, the model of the form has the lowest BIC:

```{r}
#| label: tbl-l55
#| tbl-cap: Log-likelihood, Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), residual number of degrees of freedom and number of observations used to fit a range of random effects models.

purrr::map_dfr(list(
  m_1_base,
  m_1_year,
  m_1_year_sourcetype,
  m_1_yearsource,
  m_1_year_sourcetype_nosource,
  m_1_full),
        ~(glance(.x))) %>%
  mutate(model = c(
    "1 + (1|source)",
    "scaled_year + (1|source)",
    "scaled_year + source_type +(1|source)",
    "scaled_year + (scaled_year|source)",
    "scaled_year + (1|sourcetype)",
    "scaled_year + source_type + (scaled_year|source)"
  )) %>%
  arrange(BIC) %>%
  kable()
```

We obtain a result similar to that of the simpler model:

```{r results="asis"}
report::report(m_1_year_sourcetype)
```

To summarise:

-   The effect of year was not found to be significant.
-   Relative to the Advertiser, the Age, Australian, Canberra Times, Courier Mail and Sydney Morning Herald had less frequency of condition-first language.
