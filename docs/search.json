[
  {
    "objectID": "400_analysis/02_obese.html",
    "href": "400_analysis/02_obese.html",
    "title": "Obesity phase 2",
    "section": "",
    "text": "There is interest in exploring statistics around the use of the word “obese” around the following research questions:\n\nDo tabloids use “obese” more than broadsheets?\nOr do broadsheets use “obese” more than tabloids?\nOr is there no discernible pattern of use?\n\nIs there any newspaper that uses obese more than others?\nAny year where obese is the most frequent/least frequent?\n\nAdditional question:\n\nIs there a difference of use by primary topic?\n\n\n\nTabloids use the word “obese” more frequently than broadsheets.\n\n\nMore specifically, a Welch Two Sample t-test testing the difference between the frequency in broadsheets per 1000 words and frequency in tabloids (mean of broadsheets = 3.16, mean of tabloids = 5.59) suggests that the effect is negative, statistically significant, and small (difference = -2.43, 95% CI [-2.63, -2.23], t(10062.88) = -24.01, p < .001; Cohen’s d = -0.46, 95% CI [-0.50, -0.42]).\n\n\nTabloids have shorter article lengths than broadsheets.\n\n\nMore specifically, using a Welch Two Sample t-test comparing the mean word count of articles from broadsheets and tabloids (mean of broadsheet = 778.84, mean of tabloid = 485.42) suggests that the effect is positive, statistically significant, and medium (difference = 293.42, 95% CI [273.82, 313.02], t(7348.69) = 29.34, p < .001; Cohen’s d = 0.60, 95% CI [0.56, 0.65])\n\n\nInvestigating data by year did not result in significant differences between years being observed.\nInvestigating the data by source revealed that article source explained a small amount of variance in the data, with the word obese being used less frequently in the Age, Australian, Canberra Times and Sydney Morning Herald relative to the Advertiser, while in the Hobart Mercury and Northern Territorian it was used somewhat more frequently than in the Advertiser.\nSignificant differences in the use of “obese” were observed in articles on different topics, with articles annotated as “Awards” and “Biomedical Research” using more instances per 1000 words than articles discussing politics, schooling, transport and commuting.\nSignificant differences in article topics were also observed between tabloids and broadsheets, with the topics “ChildrenParents”, “NutritionStudy”, “WomenPregnancy” and “FitnessExercise” approximately 3x more frequently reported on in tabloids than broadsheets, in contrast to topics like “Politics” and “SportsDoping”, which were approximately evenly represented between the two media types.\n\nCQPweb data was provided. To calculate normalised frequency, we divide the number of observations from CQPweb by the word count as calculated in Python, and multiple by 1000.\n\nCodelibrary(here)\nlibrary(janitor)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(knitr)\nlibrary(report)\ntheme_set(theme_minimal())\n\n\n\nCodeassess_year_source <- function(df){\n  df %>% \n  dplyr::select(source, year) %>%\n  group_by(year, source) %>% \n  count(year) %>%\n  rename(count = n) %>%\n  pivot_wider(id_cols = c(source), names_from = year, values_from = c(count), values_fill = 0) %>%\n  janitor::adorn_totals(c(\"row\", \"col\")) %>%\n  kable()\n} \nread_cqpweb <- function(filename){\n  read.csv(\n    here(\"100_data_raw\", filename), \n    skip = 3, sep = \"\\t\") %>% \n    janitor::clean_names()\n}\n\n\n\nCodeadj_obese <- read_cqpweb(\"aoc_all_obese_tagadjlemma.txt\") \nmetadata <- read_csv(here(\"100_data_raw\", \"corpus_cqpweb_metadata.csv\"))\nadditional_source_metadata <- read_csv(here(\"100_data_raw\", \"addition_source_metadata.csv\"))\ntopic_labels <- read_csv(here(\"100_data_raw\", \"topic_labels.csv\"))\nmetadata_full <- inner_join(inner_join(metadata,\n                                       topic_labels,\n                                       by = c(\"dominant_topic\" = \"topic_number\")),\n                            additional_source_metadata)\nobese_annotated <- inner_join(\n  adj_obese, metadata_full, by = c(\"text\" = \"article_id\")) %>% \n  mutate(frequency = 10^3*no_hits_in_text/wordcount_total)"
  },
  {
    "objectID": "400_analysis/02_obese.html#tabloid-vs-broadsheet",
    "href": "400_analysis/02_obese.html#tabloid-vs-broadsheet",
    "title": "Obesity phase 2",
    "section": "Tabloid vs broadsheet",
    "text": "Tabloid vs broadsheet\nWhat is the distribution of obese in articles?\n\nCodeobese_annotated %>%\n  ggplot(aes(x = no_hits_in_text, fill = source_type)) +\n  geom_bar(position = \"dodge2\") +\n  labs(\n    x = \"Number of hits in article, CQPWeb\",\n    y = \"Number of articles in corpus\",\n    fill = \"Source type\"\n  )\n\n\n\n\nHow is this usage distributed by year (number of articles in corpus)?\n\nCodeobese_annotated_filtered <- obese_annotated %>%\n  filter(!(source %in% c(\"BrisTimes\", \"Telegraph\")))\nassess_year_source(obese_annotated)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsource\n2008\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\nTotal\n\n\n\nAdvertiser\n130\n144\n115\n127\n125\n158\n122\n127\n100\n96\n77\n75\n1396\n\n\nAge\n140\n85\n107\n85\n102\n94\n88\n135\n93\n90\n63\n44\n1126\n\n\nAustralian\n115\n84\n68\n76\n58\n47\n61\n33\n35\n34\n62\n36\n709\n\n\nCanTimes\n71\n85\n70\n79\n72\n88\n98\n88\n54\n77\n65\n26\n873\n\n\nCourierMail\n167\n144\n125\n110\n97\n118\n122\n108\n100\n85\n118\n74\n1368\n\n\nHeraldSun\n196\n201\n171\n183\n138\n157\n121\n95\n84\n89\n95\n75\n1605\n\n\nHobMercury\n75\n59\n56\n57\n60\n63\n27\n33\n32\n32\n43\n38\n575\n\n\nNorthernT\n49\n54\n45\n30\n33\n31\n27\n30\n18\n20\n22\n12\n371\n\n\nSydHerald\n158\n126\n129\n130\n130\n124\n101\n142\n105\n110\n98\n81\n1434\n\n\nWestAus\n114\n96\n90\n98\n95\n77\n83\n44\n53\n27\n16\n6\n799\n\n\nBrisTimes\n0\n0\n0\n0\n0\n1\n9\n37\n5\n7\n19\n22\n100\n\n\nTelegraph\n0\n0\n0\n0\n0\n0\n93\n52\n86\n95\n79\n60\n465\n\n\nTotal\n1215\n1078\n976\n975\n910\n958\n952\n924\n765\n762\n757\n549\n10821\n\n\n\n\n\nWe can see that apart from the Brisbane times and Daily Telegraph, there are articles using “obese” every year and in every publication. We will filter out these two sources.\nHow is the frequency (per thousand words) of the usage of obese distributed by tabloids/broadsheets?\n\nCodeobese_annotated_filtered %>%\n  ggplot(aes(x = source_type, y = log(frequency))) + \n  geom_boxplot() + \n  labs(\n    x = \"Source type\",\n    y = \"log(frequency per 1000 words)\"\n  )\n\n\n\n\nAnd let’s also use a histogram to look at the distribution:\n\nCodeobese_annotated_filtered %>%\n  ggplot(aes(fill = source_type,x = log(frequency))) + \n  geom_histogram() + \n  labs(\n    fill = \"Source type\",\n    x = \"log(frequency per 1000 words)\",\n    y = \"Number of articles\"\n  )\n\n\n\n\nNote that broadsheets have somewhat longer texts than tabloids:\n\nCodeobese_annotated_filtered %>%\n  ggplot(aes(x = source_type, y = log(wordcount_total))) + \n  geom_boxplot() + \n  labs(\n    x = \"Source type\",\n    y = \"log(Python word count)\"\n  )\n\n\n\n\nLet’s use a histogram to look at the distribution in more detail:\n\nCodeobese_annotated_filtered %>%\n  ggplot(aes(fill = source_type, x = log(wordcount_total))) + \n  geom_histogram(binwidth = 0.1)+ \n  labs(\n    y = \"Number of articles\",\n    x = \"log(Python word count)\"\n  )\n\n\n\n\nThe log-transformed word count data is approximately normally distributed.\nLet’s see if the difference in length of articles using the word “obese” in tabloids and broadsheets is significant?\nCodewordcount_broadsheet <- obese_annotated_filtered[obese_annotated_filtered$source_type == \"broadsheet\",\"wordcount_total\"]\nwordcount_tabloid <- obese_annotated_filtered[obese_annotated_filtered$source_type == \"tabloid\",\"wordcount_total\"]\nreport::report(t.test(wordcount_broadsheet,wordcount_tabloid))\nThe Welch Two Sample t-test testing the difference between wordcount_broadsheet and wordcount_tabloid (mean of x = 778.84, mean of y = 485.42) suggests that the effect is positive, statistically significant, and medium (difference = 293.42, 95% CI [273.82, 313.02], t(7348.69) = 29.34, p < .001; Cohen’s d = 0.60, 95% CI [0.56, 0.65])\nYes, overall articles in tabloids are significantly shorter than in broadsheets.\nIf we then test the difference between the frequency of the word “obese” in tabloids and broadsheets, we can see that a higher frequency per 1000 words is detected for usage of the word “obese” in tabloids than broadsheets.\nCodefrequency_broadsheet <- obese_annotated_filtered[obese_annotated_filtered$source_type == \"broadsheet\",\"frequency\"]\nfrequency_tabloid <- obese_annotated_filtered[obese_annotated_filtered$source_type == \"tabloid\",\"frequency\"]\nreport::report(t.test(frequency_broadsheet, frequency_tabloid))\nThe Welch Two Sample t-test testing the difference between frequency_broadsheet and frequency_tabloid (mean of x = 3.16, mean of y = 5.59) suggests that the effect is negative, statistically significant, and small (difference = -2.43, 95% CI [-2.63, -2.23], t(10062.88) = -24.01, p < .001; Cohen’s d = -0.46, 95% CI [-0.50, -0.42])\nSo, yes, the frequency of use of the word “obese” is lower in broadsheets than in tabloids.\nLet’s use bootstrapping to see if the raw frequencies of usage of the word obese are different?\n\nCodebroadsheet_counts <- NULL\ntabloid_counts <- NULL\nfor (i in 1:10000) {\n  x <- mean(sample({obese_annotated_filtered %>% \n        filter(source_type == \"broadsheet\") %>%\n        pull(no_hits_in_text)}, 1000, replace = FALSE))\n  y <- mean(sample({obese_annotated_filtered %>% \n        filter(source_type == \"tabloid\") %>%\n        pull(no_hits_in_text)}, 1000, replace = FALSE))\n  broadsheet_counts <- c(broadsheet_counts, x)\n  tabloid_counts <- c(tabloid_counts, y)\n}\ncounts_comparison <- data.frame(\n  mean_sample = c(broadsheet_counts, tabloid_counts),\n  source_type = c(\n    rep(\"broadsheet\", length(broadsheet_counts)),\n    rep(\"tabloid\", length(tabloid_counts))))\ncounts_comparison %>%\n  ggplot(aes(x = mean_sample, fill = source_type)) + \n  geom_histogram()\n\n\n\n\nIt is interesting that the distribution of usage of “obese” is ~1.6 uses per article in tabloid publications, while the distribution for broadsheets was bimodal.\nCodereport::report(\n  t.test(\n    broadsheet_counts,\n    tabloid_counts\n  )\n)\nThe Welch Two Sample t-test testing the difference between broadsheet_counts and tabloid_counts (mean of x = 1.71, mean of y = 1.60) suggests that the effect is positive, statistically significant, and large (difference = 0.11, 95% CI [0.11, 0.11], t(18721.64) = 182.01, p < .001; Cohen’s d = 2.57, 95% CI [2.83, 2.83])"
  },
  {
    "objectID": "400_analysis/02_obese.html#differences-in-usage-by-source",
    "href": "400_analysis/02_obese.html#differences-in-usage-by-source",
    "title": "Obesity phase 2",
    "section": "Differences in usage by source",
    "text": "Differences in usage by source\nIs there a difference in the usage of obese by source?\n\nCodeobese_annotated_filtered %>%\n  ggplot(aes(x = reorder(source,frequency), \n             y = log(frequency), \n             fill = source_type)) + \n  geom_boxplot() +\n  theme(axis.text.x = \n          element_text(angle = 90, vjust = 0.5, hjust=1),\n        legend.position = \"NA\")\n\n\n\n\nThe visualisation suggests there are not - only the differences observed above for tabloids vs broadsheets.\nWhat are the means and standard deviations of the frequency by source?\n\nCodeobese_annotated_filtered %>% \n  group_by(source) %>%\n  summarise(\n    mean = mean(frequency),\n    median = median(frequency),\n    sd = sd(frequency),\n    type = (source_type)\n  ) %>%\n  distinct() %>%\n  arrange(mean) %>%\n  kable()\n\n\n\nsource\nmean\nmedian\nsd\ntype\n\n\n\nAustralian\n2.817048\n1.792115\n3.175300\nbroadsheet\n\n\nSydHerald\n3.072108\n1.821494\n3.657049\nbroadsheet\n\n\nAge\n3.154595\n1.828154\n4.060922\nbroadsheet\n\n\nCanTimes\n3.569830\n2.200220\n4.025436\nbroadsheet\n\n\nWestAus\n5.292246\n3.397508\n5.496833\ntabloid\n\n\nCourierMail\n5.351845\n3.514949\n5.800857\ntabloid\n\n\nHeraldSun\n5.493390\n3.194888\n6.965477\ntabloid\n\n\nAdvertiser\n5.565510\n3.120132\n6.236993\ntabloid\n\n\nNorthernT\n6.251933\n4.385965\n6.097086\ntabloid\n\n\nHobMercury\n6.464202\n3.802281\n8.143909\ntabloid\n\n\n\n\n\nIt seems that within the different sources among broadsheets and tabloids there is not much difference among the frequency of use of the word “obese”."
  },
  {
    "objectID": "400_analysis/02_obese.html#differences-in-usage-by-year",
    "href": "400_analysis/02_obese.html#differences-in-usage-by-year",
    "title": "Obesity phase 2",
    "section": "Differences in usage by year",
    "text": "Differences in usage by year\nIs there any year when “obese” is used more frequently than others?\n\nCodeobese_annotated_filtered %>%\n  ggplot(aes(x = reorder(year,frequency), \n             y = log(frequency), \n             fill = year)) + \n  geom_boxplot() +\n  theme(axis.text.x = \n          element_text(angle = 90, vjust = 0.5, hjust=1),\n        legend.position = \"NA\")\n\n\n\n\nBased on the visualisation it seems not. If we separate out by source there also doesn’t seem to be much difference. If we use a jitter plot to visualise the data, then fit a smoothing line and compare with the line of “no change” (dashed red line), we can see that there really isn’t much of a difference by source and year:\n\nCodeobese_annotated_filtered %>%\n  ggplot(aes(x = as.factor(year), \n             y = log(frequency), \n             fill = year)) + \n  geom_jitter(alpha = 0.2) +\n  geom_smooth(aes(group = source), col = \"blue\", method = \"loess\") +\n  geom_hline(yintercept = 1, col = \"red\", lty = 3, size = 3) + \n  facet_wrap(~source) + \n  theme(axis.text.x = \n          element_text(angle = 90, vjust = 0.5, hjust=1),\n        legend.position = \"NA\") + \n  labs(\n    x = \"Year\",\n    y = \"log(frequency per 1000 words)\"\n  )\n\n\n\n\nThe distributions each year also look quite similar:\n\nCodeobese_annotated_filtered %>%\n  ggplot(aes(x = reorder(year,frequency), \n             y = log(frequency), \n             fill = year)) + \n  geom_violin() +\n  theme(axis.text.x = \n          element_text(angle = 90, vjust = 0.5, hjust=1),\n        legend.position = \"NA\")\n\n\n\n\n\nCodeobese_annotated_filtered %>% \n  group_by(year) %>%\n  summarise(\n    mean = mean(frequency),\n    median = median(frequency),\n    sd = sd(frequency),\n    year = (year)\n  ) %>%\n  distinct() %>%\n  arrange(mean) %>%\n  kable()\n\n\n\nyear\nmean\nmedian\nsd\n\n\n\n2019\n3.817842\n2.475248\n3.971667\n\n\n2017\n3.866365\n2.389490\n4.300584\n\n\n2018\n3.966531\n2.409639\n4.616181\n\n\n2015\n4.335848\n2.531646\n4.950555\n\n\n2016\n4.471581\n2.649011\n5.673864\n\n\n2008\n4.512463\n2.732240\n4.882211\n\n\n2013\n4.581882\n2.793296\n5.057143\n\n\n2009\n4.801149\n2.706365\n6.176861\n\n\n2014\n4.926691\n2.840909\n6.018102\n\n\n2010\n4.938758\n2.743489\n7.376403\n\n\n2012\n5.065254\n2.816924\n6.234889\n\n\n2011\n5.128600\n2.865330\n6.218089"
  },
  {
    "objectID": "400_analysis/02_obese.html#differences-in-usage-by-source-type-source-and-year",
    "href": "400_analysis/02_obese.html#differences-in-usage-by-source-type-source-and-year",
    "title": "Obesity phase 2",
    "section": "Differences in usage by source type, source and year",
    "text": "Differences in usage by source type, source and year\nWe can also try to simultaneously model differences by source type, source and year.\nAs expected, a model that includes the source type (tabloid vs broadsheet) gives a better fit than one that does not:\n\nCodeobese_annotated_filtered$scaled_year <- scale(\n  obese_annotated_filtered$year, scale = F)\nlibrary(lme4)\n# base model\nm_0_base <- lm(log(frequency) ~ 1, \n                data = obese_annotated_filtered)\n# with source type\nm_0_sourcetype <- lm(log(frequency) ~ source_type,  \n                data = obese_annotated_filtered)\n# with year\nm_0_year <- lm(log(frequency) ~ scaled_year,  \n                data = obese_annotated_filtered)\n# with source \nm_0_source <- lm(log(frequency) ~ source,  \n                data = obese_annotated_filtered)\n# compare\nrbind({broom::glance(m_0_base) %>% \n    dplyr::select(-df.residual,- deviance, -nobs) %>%\n    mutate(model = \"1\")},\n    {broom::glance(m_0_sourcetype)%>% \n          dplyr::select(-df.residual,- deviance, -nobs) %>%\n        mutate(model = \"source_type\")},\n    {broom::glance(m_0_year) %>% \n    dplyr::select(-df.residual,- deviance, -nobs) %>%\n    mutate(model = \"scaled_year\")},\n    {broom::glance(m_0_source) %>% \n    dplyr::select(-df.residual,- deviance, -nobs) %>%\n    mutate(model = \"source\")}\n) %>% \n  dplyr::select(model, everything()) %>%\n  arrange(AIC) %>%\n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\n\n\n\nsource\n0.0952110\n0.0944163\n0.8696337\n119.79863\n0e+00\n9\n-13115.04\n26252.08\n26331.67\n\n\nsource_type\n0.0891430\n0.0890541\n0.8722045\n1003.52945\n0e+00\n1\n-13149.32\n26304.63\n26326.34\n\n\nscaled_year\n0.0026045\n0.0025072\n0.9126977\n26.77577\n2e-07\n1\n-13614.74\n27235.48\n27257.19\n\n\n1\n0.0000000\n0.0000000\n0.9138440\nNA\nNA\nNA\n-13628.12\n27260.23\n27274.70\n\n\n\n\n\nWe can see that the model incorporating source provides the best fit for the data, explaining somewhat more variability than that which includes only source type.\n\nCodeanova(m_0_sourcetype, m_0_source)\n\nAnalysis of Variance Table\nModel 1: log(frequency) ~ source_type\nModel 2: log(frequency) ~ source\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n1  10254 7800.6                                  \n2  10246 7748.7  8    51.967 8.5895 9.738e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNext, let’s compare if adding the source in as an random variable improves the fit of the model.\n\nCodem_1_source <- lmer(log(frequency) ~  1 + (1|source), \n                data = obese_annotated_filtered, REML = F)\nrbind(\n    {broom::glance(m_0_source)%>% \n          dplyr::select(AIC, BIC, logLik) %>%\n        mutate(model = \"1 + source\")},\n    {broom.mixed::glance(m_1_source) %>% \n    dplyr::select(AIC, BIC, logLik) %>%\n    mutate(model = \"1 + (1|source)\")}\n) \n\n# A tibble: 2 × 4\n     AIC    BIC  logLik model         \n   <dbl>  <dbl>   <dbl> <chr>         \n1 26252. 26332. -13115. 1 + source    \n2 26293. 26315. -13144. 1 + (1|source)\n\n\nIncluding a random intercept for source does not improve the fit of the model (AIC increases, logLik decreases).\nIn summary, the model which included source only provided the best fit. Let’s summarise this model:\nCodereport::report(m_0_source)\nWe fitted a linear model (estimated using OLS) to predict frequency with source (formula: log(frequency) ~ source). The model explains a statistically significant and weak proportion of variance (R2 = 0.10, F(9, 10246) = 119.80, p < .001, adj. R2 = 0.09). The model’s intercept, corresponding to source = Advertiser, is at 1.26 (95% CI [1.22, 1.31], t(10246) = 54.27, p < .001). Within this model:\n\nThe effect of source [Age] is statistically significant and negative (beta = -0.55, 95% CI [-0.62, -0.48], t(10246) = -15.81, p < .001; Std. beta = -0.21, 95% CI [-0.23, -0.18])\nThe effect of source [Australian] is statistically significant and negative (beta = -0.62, 95% CI [-0.70, -0.54], t(10246) = -15.54, p < .001; Std. beta = -0.23, 95% CI [-0.26, -0.20])\nThe effect of source [CanTimes] is statistically significant and negative (beta = -0.37, 95% CI [-0.45, -0.30], t(10246) = -9.93, p < .001; Std. beta = -0.16, 95% CI [-0.19, -0.13])\nThe effect of source [CourierMail] is statistically non-significant and positive (beta = 7.98e-03, 95% CI [-0.06, 0.07], t(10246) = 0.24, p = 0.809; Std. beta = -5.87e-03, 95% CI [-0.03, 0.02])\nThe effect of source [HeraldSun] is statistically non-significant and positive (beta = 0.02, 95% CI [-0.04, 0.08], t(10246) = 0.66, p = 0.507; Std. beta = -5.71e-03, 95% CI [-0.03, 0.02])\nThe effect of source [HobMercury] is statistically significant and positive (beta = 0.15, 95% CI [0.06, 0.23], t(10246) = 3.38, p < .001; Std. beta = 0.06, 95% CI [0.02, 0.10])\nThe effect of source [NorthernT] is statistically significant and positive (beta = 0.23, 95% CI [0.13, 0.33], t(10246) = 4.59, p < .001; Std. beta = 0.07, 95% CI [0.03, 0.12])\nThe effect of source [SydHerald] is statistically significant and negative (beta = -0.52, 95% CI [-0.59, -0.46], t(10246) = -16.03, p < .001; Std. beta = -0.21, 95% CI [-0.23, -0.18])\nThe effect of source [WestAus] is statistically non-significant and positive (beta = 0.03, 95% CI [-0.04, 0.11], t(10246) = 0.90, p = 0.366; Std. beta = -5.60e-03, 95% CI [-0.04, 0.03])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\nTo summarise, a model was fit by source, which explained a small amount of variance in the data. It showed that the Age, Australian, Canberra Times and Sydney Morning Herald had a lower frequency of use of the word “obese” relative to the Advertiser, while in the Hobart Mercury, Northern Territorian the word “obese” was used somewhat more frequently than in the Advertiser."
  },
  {
    "objectID": "400_analysis/02_obese.html#differences-in-usage-by-topic",
    "href": "400_analysis/02_obese.html#differences-in-usage-by-topic",
    "title": "Obesity phase 2",
    "section": "Differences in usage by topic",
    "text": "Differences in usage by topic\n\nCodemetadata_full %>%\n  group_by(source_type) %>%\n  count(topic_label) %>%\n  pivot_wider(names_from = source_type, values_from = n) %>%\n  mutate(\n    total_topic = broadsheet + tabloid,\n    prop_broadsheet = round(100*broadsheet/total_topic, 2)\n  ) %>% \n  arrange(prop_broadsheet) %>%\n  kable()\n\n\n\n\n\n\n\n\n\n\ntopic_label\nbroadsheet\ntabloid\ntotal_topic\nprop_broadsheet\n\n\n\nChildrenParents\n364\n1335\n1699\n21.42\n\n\nNutritionStudy\n642\n1797\n2439\n26.32\n\n\nWomenPregnancy\n159\n385\n544\n29.23\n\n\nFitnessExercise\n464\n1093\n1557\n29.80\n\n\nBiomedResearch\n1013\n2077\n3090\n32.78\n\n\nMedicalHealth\n297\n524\n821\n36.18\n\n\nFood\n539\n800\n1339\n40.25\n\n\nFastFood&Drinks\n1044\n1441\n2485\n42.01\n\n\nTransport&Commuting\n427\n559\n986\n43.31\n\n\nWomenGirls\n625\n744\n1369\n45.65\n\n\nAwards\n28\n33\n61\n45.90\n\n\nPublicHealthReport\n1243\n1338\n2581\n48.16\n\n\nStudents&Teachers\n536\n572\n1108\n48.38\n\n\nMicrobiome\n32\n33\n65\n49.23\n\n\nSportsDoping\n260\n253\n513\n50.68\n\n\nPolitics\n1478\n1249\n2727\n54.20\n\n\nMusicMovies\n1543\n1236\n2779\n55.52\n\n\n\n\n\nWe can see that articles labelled with the topics “ChildrenParents”, “NutritionStudy”, “WomenPregnancy” and “FitnessExercise” are approximately 3x more frequently reported on in tabloids than broadsheets, in contrast to topics like “Politics” and “SportsDoping” which are approximately evenly represented between the two media types.\n\nCodeobese_annotated_filtered %>%\n  ggplot(aes(x = as.factor(\n    reorder(topic_label,frequency)), \n             y = log(frequency), \n             fill = topic_label)) + \n  geom_boxplot() +\n  theme(axis.text.x = \n          element_text(angle = 90, vjust = 0.5, hjust=1),\n        legend.position = \"NA\") +\n  labs(x = \"Topic label\",\n       y = \"log(frequency per 1000 words)\")\n\n\n\n\nIt seems there are some topics that use obese more than others.\nWe use a simple linear model with post-hoc comparisons and Bonferroni multiple testing correction:\n\nCodeobese_by_topic <- lm(\n  frequency ~ as.factor(topic_label), \n  data = obese_annotated_filtered)\nlibrary(emmeans)\nobese_by_topic_comp <- emmeans(obese_by_topic, pairwise ~ as.factor(topic_label), adjust = \"bonferroni\")\nobese_by_topic_comp$contrasts %>%\n    summary(infer = TRUE) %>%\n  filter(p.value < 0.01) %>% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\nAwards - Food\n5.942155\n1.4853012\n10239\n0.6493259\n11.2349838\n4.000640\n0.0086518\n\n\nAwards - Politics\n6.288435\n1.4729512\n10239\n1.0396145\n11.5372546\n4.269276\n0.0026911\n\n\nAwards - Students&Teachers\n6.172930\n1.4968448\n10239\n0.8389660\n11.5068943\n4.123961\n0.0051048\n\n\nAwards - Transport&Commuting\n5.942849\n1.4971190\n10239\n0.6079082\n11.2777906\n3.969524\n0.0098612\n\n\nBiomedResearch - ChildrenParents\n2.635277\n0.2323342\n10239\n1.8073605\n3.4631932\n11.342613\n0.0000000\n\n\nBiomedResearch - FastFood&Drinks\n4.493757\n0.2333711\n10239\n3.6621454\n5.3253683\n19.255838\n0.0000000\n\n\nBiomedResearch - FitnessExercise\n3.872260\n0.2219288\n10239\n3.0814229\n4.6630966\n17.448211\n0.0000000\n\n\nBiomedResearch - Food\n5.623753\n0.2991564\n10239\n4.5577179\n6.6897886\n18.798706\n0.0000000\n\n\nBiomedResearch - MedicalHealth\n4.461614\n0.3299740\n10239\n3.2857608\n5.6374669\n13.521107\n0.0000000\n\n\nBiomedResearch - Microbiome\n5.446934\n1.1570708\n10239\n1.3237445\n9.5701229\n4.707520\n0.0003455\n\n\nBiomedResearch - MusicMovies\n5.496256\n0.1920506\n10239\n4.8118893\n6.1806227\n28.618798\n0.0000000\n\n\nBiomedResearch - NutritionStudy\n3.717134\n0.2237552\n10239\n2.9197880\n4.5144790\n16.612498\n0.0000000\n\n\nBiomedResearch - Politics\n5.970033\n0.2301306\n10239\n5.1499692\n6.7900968\n25.941938\n0.0000000\n\n\nBiomedResearch - PublicHealthReport\n1.510443\n0.2134401\n10239\n0.7498552\n2.2710307\n7.076659\n0.0000000\n\n\nBiomedResearch - Students&Teachers\n5.854529\n0.3520215\n10239\n4.6001098\n7.1089473\n16.631166\n0.0000000\n\n\nBiomedResearch - Transport&Commuting\n5.624448\n0.3531856\n10239\n4.3658810\n6.8830145\n15.924909\n0.0000000\n\n\nBiomedResearch - WomenGirls\n5.197677\n0.2446344\n10239\n4.3259286\n6.0694243\n21.246710\n0.0000000\n\n\nChildrenParents - FastFood&Drinks\n1.858480\n0.2705723\n10239\n0.8943035\n2.8226567\n6.868701\n0.0000000\n\n\nChildrenParents - FitnessExercise\n1.236983\n0.2607674\n10239\n0.3077457\n2.1662201\n4.743626\n0.0002894\n\n\nChildrenParents - Food\n2.988476\n0.3290012\n10239\n1.8160898\n4.1608630\n9.083481\n0.0000000\n\n\nChildrenParents - MedicalHealth\n1.826337\n0.3572536\n10239\n0.5532740\n3.0994000\n5.112159\n0.0000441\n\n\nChildrenParents - MusicMovies\n2.860979\n0.2358616\n10239\n2.0204932\n3.7014651\n12.129909\n0.0000000\n\n\nChildrenParents - NutritionStudy\n1.081857\n0.2623236\n10239\n0.1470739\n2.0166394\n4.124130\n0.0051011\n\n\nChildrenParents - Politics\n3.334756\n0.2677823\n10239\n2.3805216\n4.2889907\n12.453238\n0.0000000\n\n\nChildrenParents - PublicHealthReport\n-1.124834\n0.2535822\n10239\n-2.0284669\n-0.2212008\n-4.435776\n0.0012607\n\n\nChildrenParents - Students&Teachers\n3.219252\n0.3777121\n10239\n1.8732854\n4.5652181\n8.523031\n0.0000000\n\n\nChildrenParents - Transport&Commuting\n2.989171\n0.3787972\n10239\n1.6393379\n4.3390039\n7.891218\n0.0000000\n\n\nChildrenParents - WomenGirls\n2.562400\n0.2803449\n10239\n1.5633986\n3.5614007\n9.140169\n0.0000000\n\n\nChildrenParents - WomenPregnancy\n-2.125124\n0.3656700\n10239\n-3.4281787\n-0.8220691\n-5.811589\n0.0000009\n\n\nFastFood&Drinks - MusicMovies\n1.002499\n0.2368831\n10239\n0.1583730\n1.8466252\n4.232042\n0.0031769\n\n\nFastFood&Drinks - Politics\n1.476276\n0.2686824\n10239\n0.5188338\n2.4337183\n5.494502\n0.0000055\n\n\nFastFood&Drinks - PublicHealthReport\n-2.983314\n0.2545326\n10239\n-3.8903337\n-2.0762942\n-11.720753\n0.0000000\n\n\nFastFood&Drinks - WomenPregnancy\n-3.983604\n0.3663297\n10239\n-5.2890096\n-2.6781982\n-10.874367\n0.0000000\n\n\nFitnessExercise - Food\n1.751494\n0.3217375\n10239\n0.6049911\n2.8979959\n5.443859\n0.0000073\n\n\nFitnessExercise - MusicMovies\n1.623996\n0.2256189\n10239\n0.8200098\n2.4279827\n7.197963\n0.0000000\n\n\nFitnessExercise - Politics\n2.097773\n0.2588060\n10239\n1.1755254\n3.0200210\n8.105582\n0.0000000\n\n\nFitnessExercise - PublicHealthReport\n-2.361817\n0.2440843\n10239\n-3.2316042\n-1.4920294\n-9.676235\n0.0000000\n\n\nFitnessExercise - Students&Teachers\n1.982269\n0.3714022\n10239\n0.6587875\n3.3057501\n5.337256\n0.0000131\n\n\nFitnessExercise - Transport&Commuting\n1.752188\n0.3725057\n10239\n0.4247745\n3.0796015\n4.703789\n0.0003519\n\n\nFitnessExercise - WomenGirls\n1.325417\n0.2717838\n10239\n0.3569227\n2.2939107\n4.876731\n0.0001489\n\n\nFitnessExercise - WomenPregnancy\n-3.362107\n0.3591487\n10239\n-4.6419229\n-2.0822907\n-9.361323\n0.0000000\n\n\nFood - NutritionStudy\n-1.906620\n0.3230001\n10239\n-3.0576213\n-0.7556181\n-5.902846\n0.0000005\n\n\nFood - PublicHealthReport\n-4.113310\n0.3159419\n10239\n-5.2391605\n-2.9874601\n-13.019197\n0.0000000\n\n\nFood - WomenPregnancy\n-5.113600\n0.4113757\n10239\n-6.5795258\n-3.6476747\n-12.430488\n0.0000000\n\n\nMedicalHealth - Politics\n1.508419\n0.3558244\n10239\n0.2404489\n2.7763894\n4.239223\n0.0030772\n\n\nMedicalHealth - PublicHealthReport\n-2.951171\n0.3452646\n10239\n-4.1815114\n-1.7208304\n-8.547564\n0.0000000\n\n\nMedicalHealth - WomenPregnancy\n-3.951461\n0.4343020\n10239\n-5.4990839\n-2.4038379\n-9.098417\n0.0000000\n\n\nMicrobiome - WomenPregnancy\n-4.936781\n1.1910282\n10239\n-9.1809762\n-0.6925853\n-4.144974\n0.0046592\n\n\nMusicMovies - NutritionStudy\n-1.779122\n0.2274157\n10239\n-2.5895120\n-0.9687330\n-7.823217\n0.0000000\n\n\nMusicMovies - PublicHealthReport\n-3.985813\n0.2172744\n10239\n-4.7600643\n-3.2115618\n-18.344603\n0.0000000\n\n\nMusicMovies - WomenPregnancy\n-4.986103\n0.3414950\n10239\n-6.2030107\n-3.7691954\n-14.600810\n0.0000000\n\n\nNutritionStudy - Politics\n2.252899\n0.2603739\n10239\n1.3250644\n3.1807345\n8.652555\n0.0000000\n\n\nNutritionStudy - PublicHealthReport\n-2.206691\n0.2457462\n10239\n-3.0824000\n-1.3309811\n-8.979553\n0.0000000\n\n\nNutritionStudy - Students&Teachers\n2.137395\n0.3724965\n10239\n0.8100143\n3.4647759\n5.738027\n0.0000013\n\n\nNutritionStudy - Transport&Commuting\n1.907314\n0.3735967\n10239\n0.5760128\n3.2386157\n5.105275\n0.0000457\n\n\nNutritionStudy - WomenGirls\n1.480543\n0.2732773\n10239\n0.5067270\n2.4543590\n5.417731\n0.0000084\n\n\nNutritionStudy - WomenPregnancy\n-3.206980\n0.3602802\n10239\n-4.4908287\n-1.9231323\n-8.901352\n0.0000000\n\n\nPolitics - PublicHealthReport\n-4.459590\n0.2515648\n10239\n-5.3560340\n-3.5631460\n-17.727402\n0.0000000\n\n\nPolitics - WomenPregnancy\n-5.459880\n0.3642739\n10239\n-6.7579598\n-4.1618003\n-14.988392\n0.0000000\n\n\nPublicHealthReport - Students&Teachers\n4.344086\n0.3663931\n10239\n3.0384541\n5.6497171\n11.856351\n0.0000000\n\n\nPublicHealthReport - Transport&Commuting\n4.114005\n0.3675116\n10239\n2.8043875\n5.4236221\n11.194217\n0.0000000\n\n\nPublicHealthReport - WomenGirls\n3.687234\n0.2648976\n10239\n2.7432783\n4.6311888\n13.919465\n0.0000000\n\n\nStudents&Teachers - WomenPregnancy\n-5.344376\n0.4512810\n10239\n-6.9525027\n-3.7362485\n-11.842678\n0.0000000\n\n\nTransport&Commuting - WomenPregnancy\n-5.114295\n0.4521896\n10239\n-6.7256596\n-3.5029300\n-11.310067\n0.0000000\n\n\nWomenGirls - WomenPregnancy\n-4.687524\n0.3736059\n10239\n-6.0188577\n-3.3561893\n-12.546705\n0.0000000\n\n\n\n\n\nWe can see that there are differences in the frequency of use of the word “obese” by topic, with articles annotated as “Awards” and “Biomedical Research” using more instances per 1000 words than articles discussing politics, schooling, transport and commuting."
  },
  {
    "objectID": "400_analysis/01_condition_person_first.html",
    "href": "400_analysis/01_condition_person_first.html",
    "title": "Obesity phase 2",
    "section": "",
    "text": "In this notebook, we explore whether there is a difference in the use of condition- vs person-first language in the Australian obesity corpus.\n\n\nCondition-first language is used in 9-14% of articles from all sources, while person-first language is used in less than 1% of articles.\nCondition-first language is used in 7-14% of articles per year across the study time period, while person-first language is used in 0.17-1.14% of articles across per year.\nPerson-first language is present in approximately the same number of articles in broadsheet and tabloid newspapers, whereas articles with only condition-first language are higher in number in tabloid publications.\n\n\nThe Pearson’s Chi-squared test with Yates’ continuity correction contrasting articles from tabloids and broadsheets that use only condition-first vs only person-first language indicate a significant link (X-squared = 4.8274, p-value = 0.02801) between type of publication and number of articles using a specific language type. The effect size is quite small (<0.2), indicating that while the result is statistically significant, the fields are weakly associated.\n\n\nPerson-first language is present in approximately the same number of articles in left- and right-leaning newspapers, whereas articles with only condition-first language are higher in number in right-leaning publications.\n\n\nThe Pearson’s Chi-squared test with Yates’ continuity correction contrasting articles from left- and right-leaning publications that use only condition-first vs only person-first language indicate a significant link (X-squared = 4.6405, p-value = 0.03123) between type of publication and number of articles using a specific language type. The effect size is, however, also negligible, indicating that while the result is statistically significant, the fields are weakly associated.\n\n\nResampling the corpus 10000 times to select 1000 articles at a time without replacement results in a mean of 4 articles per 1000 using person-first language, and 122 per 1000 using condition-first language.\n\n\nThe Welch Two Sample t-test testing the difference between person_first and condition_first bootstrapping (mean of person_first = 4.11, mean of condition_first = 122.57) suggests that the effect is negative, statistically significant, and large (difference = -118.46, 95% CI [-118.67, -118.26], t(10751.55) = -1138.14, p < .001; Cohen’s d = -16.10, 95% CI [-16.31, -15.88]).\n\n\nIn texts that use either condition-first, person-first languages or both, the frequency of condition-first language is higher (mean 4 words per 1000) than person-first (mean 2.7 words per 1000).\n\n\nThe Welch Two Sample t-test testing the difference between condition_first_frequencies and person_first_frequencies (mean of x = 4.34, mean of y = 2.67) suggests that the effect is positive, statistically significant, and small (difference = 1.66, 95% CI [1.16, 2.17], t(131.59) = 6.49, p < .001; Cohen’s d = 0.44, 95% CI [0.30, 0.58])\n\n\nRelative to the Advertiser, the Age, Australian, Canberra Times, Courier Mail and Sydney Morning Herald had lower frequency of condition-first language.\n\n\nCodelibrary(here)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(ggvenn)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(knitr)\nlibrary(ggrepel)\nlibrary(report)\nlibrary(lme4)\nlibrary(optimx)\n# set ggplot2 to use the minimal theme for all figures in the document\n# unless explicitly specified otherwise\ntheme_set(theme_minimal())\nread_cqpweb <- function(filename){\n  read.csv(\n    here(\"100_data_raw\", filename), \n    skip = 3, sep = \"\\t\") %>% \n    janitor::clean_names()\n}\ncondition_first <- read_cqpweb(\"aoc_all_condition_first.txt\")\nperson_first <- read_cqpweb(\"aoc_all_person_first.txt\")\nmetadata <- read_csv(here(\"100_data_raw\", \"corpus_cqpweb_metadata.csv\"))\nadditional_source_metadata <- read_csv(here(\"100_data_raw\", \"addition_source_metadata.csv\"))\nmetadata_full <- inner_join(metadata, additional_source_metadata)\ncondition_first_annotated <- inner_join(\n  metadata_full, condition_first, by = c(\"article_id\" = \"text\")) %>% \n  mutate(frequency = 10^3*no_hits_in_text/wordcount_total) \nperson_first_annotated <- inner_join(\n  metadata_full, person_first, by = c(\"article_id\" = \"text\"))%>% \n  mutate(frequency = 10^3*no_hits_in_text/wordcount_total) \ncorpus_articlecounts <- read_csv(here(\"100_data_raw\", \"articlecounts_full.csv\"), col_names = TRUE, skip = 1) %>% filter(year != \"source\") %>% rename(source = year)\n\n\nAs discussed in the EDA, we use the Python-generated word counts to count the frequency of occurrences per thousand words, as these do not include counts for punctuation symbols and hence do not distort counts for longer texts.\n\nFirst, we explore how many articles (absolute numbers and relative to the total number of articles in each source) use condition-first vs person-first language?\nThis table shows how the number and percentage (out of 100%) of articles in which person-first and condition-first language is used in the corpus, by publication:\n\nCodecondition_person_rbound <-\n  person_first_annotated %>%\n  select(article_id, year, source) %>%\n  mutate(type = \"Person-first\") %>%\n  rbind({\n  condition_first_annotated %>%\n  select(article_id, year, source) %>%\n  mutate(type = \"Condition-first\")  \n  })\n# generate how many articles per source are in the corpus\ncorpus_total_articles_bysource <-\n  corpus_articlecounts %>% \n  rowwise() %>% \n  mutate(total = sum(c_across(where(is.numeric)))) %>% \n  select(source, total)\ncondition_person_rbound %>% \n  select(-year) %>% \n  group_by(type) %>% \n  count(source) %>%\n  inner_join(corpus_total_articles_bysource) %>%\n  mutate(percent = round(100*n/total, 2)) %>%\n  rename(count = n) %>%\n  pivot_wider(id_cols = source, names_from = type, values_from = c(count, total, percent), names_glue = \"{type} {.value}\") %>%\n  rename(Total_articles = `Person-first total`) %>%\n  select(-`Condition-first total`) %>%\n  kable()\n\n\n\n\n\n\n\n\n\n\n\nsource\nCondition-first count\nPerson-first count\nTotal_articles\nCondition-first percent\nPerson-first percent\n\n\n\nAdvertiser\n456\n8\n3349\n13.62\n0.24\n\n\nAge\n315\n19\n2826\n11.15\n0.67\n\n\nAustralian\n191\n8\n1960\n9.74\n0.41\n\n\nBrisTimes\n22\n1\n228\n9.65\n0.44\n\n\nCanTimes\n212\n8\n2044\n10.37\n0.39\n\n\nCourierMail\n434\n12\n3131\n13.86\n0.38\n\n\nHeraldSun\n509\n14\n3722\n13.68\n0.38\n\n\nHobMercury\n172\n2\n1465\n11.74\n0.14\n\n\nNorthernT\n95\n2\n822\n11.56\n0.24\n\n\nSydHerald\n430\n23\n3636\n11.83\n0.63\n\n\nTelegraph\n144\n6\n1089\n13.22\n0.55\n\n\nWestAus\n228\n3\n1891\n12.06\n0.16\n\n\n\n\n\nWe can see that condition-first language is used in 9-14% of articles, while person-first language is used in less than 1% of articles accross all sources.\nThis table shows how the number and percentage (out of 100%) of articles in which person-first and condition-first language is used in the corpus, by year:\n\nCode# generate how many articles per year are in the corpus\ncorpus_total_articles_byyear <-\n  corpus_articlecounts %>%\n  pivot_longer(cols = -source, names_to = \"year\", values_to = \"number_of_articles\" ) %>%\n  select(-source) %>%\n  group_by(year) %>%\n  summarise(total = sum(number_of_articles)) %>%\n  mutate(year = as.numeric(year))\n# count the number of articles per source that use person first language\ncondition_person_rbound %>% \n  select(-source) %>% \n  group_by(type) %>% \n  count(year) %>%\n  inner_join(corpus_total_articles_byyear) %>%\n  mutate(percent = round(100*n/total, 2)) %>%\n  rename(count = n) %>%\n  pivot_wider(id_cols = year, names_from = type, values_from = c(count, total, percent), names_glue = \"{type} {.value}\") %>%\n  rename(Total_articles = `Person-first total`) %>%\n  select(-`Condition-first total`) %>%\n  kable()\n\n\n\n\n\n\n\n\n\n\n\nyear\nCondition-first count\nPerson-first count\nTotal_articles\nCondition-first percent\nPerson-first percent\n\n\n\n2008\n398\n6\n3000\n13.27\n0.20\n\n\n2009\n348\n6\n2472\n14.08\n0.24\n\n\n2010\n304\n4\n2394\n12.70\n0.17\n\n\n2011\n295\n5\n2245\n13.14\n0.22\n\n\n2012\n289\n5\n2162\n13.37\n0.23\n\n\n2013\n283\n11\n2620\n10.80\n0.42\n\n\n2014\n283\n8\n2219\n12.75\n0.36\n\n\n2015\n256\n8\n2265\n11.30\n0.35\n\n\n2016\n248\n15\n1829\n13.56\n0.82\n\n\n2017\n201\n16\n1791\n11.22\n0.89\n\n\n2018\n196\n6\n1765\n11.10\n0.34\n\n\n2019\n107\n16\n1401\n7.64\n1.14\n\n\n\n\n\nWe can see that condition-first language is used in 7-14% of articles per year across the study time period, while person-first language is used in 0.17-1.14% of articles across per year.\nFurthermore, the numbers of articles that use person-first language within the corpus are quite small, so we cannot simultaneously explore whether this type of language changes across both publication and year:\n\nCodeassess_year_source <- function(df){\n  df %>% \n  select(source, year) %>%\n  group_by(year, source) %>% \n  count(year) %>%\n  rename(count = n) %>%\n  pivot_wider(id_cols = c(source), names_from = year, values_from = c(count), values_fill = 0) %>%\n  janitor::adorn_totals(c(\"row\", \"col\")) %>%\n  kable()\n} \nassess_year_source(person_first_annotated) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsource\n2008\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\nTotal\n\n\n\nAdvertiser\n1\n1\n0\n0\n1\n1\n0\n0\n1\n2\n0\n1\n8\n\n\nAge\n2\n0\n1\n2\n2\n1\n2\n3\n2\n2\n1\n1\n19\n\n\nCourierMail\n1\n0\n1\n0\n0\n1\n2\n1\n4\n1\n1\n0\n12\n\n\nHeraldSun\n1\n0\n0\n2\n0\n3\n1\n0\n2\n1\n1\n3\n14\n\n\nSydHerald\n1\n2\n1\n1\n1\n2\n1\n2\n1\n3\n1\n7\n23\n\n\nAustralian\n0\n1\n0\n0\n0\n1\n0\n0\n1\n2\n1\n2\n8\n\n\nCanTimes\n0\n1\n0\n0\n1\n2\n0\n1\n0\n1\n0\n2\n8\n\n\nHobMercury\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n2\n\n\nWestAus\n0\n0\n1\n0\n0\n0\n0\n0\n1\n1\n0\n0\n3\n\n\nNorthernT\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n2\n\n\nTelegraph\n0\n0\n0\n0\n0\n0\n1\n0\n2\n2\n1\n0\n6\n\n\nBrisTimes\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n\n\nTotal\n6\n6\n4\n5\n5\n11\n8\n8\n15\n16\n6\n16\n106\n\n\n\n\n\nThere is also not a lot of articles that use such language from each publication (2-23 articles, mean 9.9 +/- 7.14), so modelling the trend by publication is unlikely to result in meaningful data.\nWe do have a reasonable number of articles that use condition-first language, so we can model this if desired (except for the Brisbane Times and Daily Telegraph, where we are missing data prior to 2014):\n\nCodeassess_year_source(condition_first_annotated) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsource\n2008\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\nTotal\n\n\n\nAdvertiser\n40\n53\n43\n49\n40\n51\n43\n41\n37\n25\n19\n15\n456\n\n\nAge\n49\n24\n24\n28\n37\n25\n22\n34\n26\n18\n19\n9\n315\n\n\nAustralian\n35\n26\n20\n22\n17\n11\n16\n8\n9\n7\n12\n8\n191\n\n\nCanTimes\n15\n18\n14\n18\n24\n17\n31\n24\n18\n10\n19\n4\n212\n\n\nCourierMail\n65\n44\n50\n31\n25\n40\n38\n36\n29\n29\n28\n19\n434\n\n\nHeraldSun\n66\n71\n56\n61\n51\n49\n35\n25\n32\n28\n22\n13\n509\n\n\nHobMercury\n31\n16\n15\n20\n14\n23\n8\n13\n8\n8\n11\n5\n172\n\n\nNorthernT\n12\n21\n13\n5\n7\n10\n7\n5\n4\n7\n3\n1\n95\n\n\nSydHerald\n47\n52\n41\n35\n38\n41\n30\n37\n36\n23\n32\n18\n430\n\n\nWestAus\n38\n23\n28\n26\n36\n16\n17\n14\n17\n9\n3\n1\n228\n\n\nBrisTimes\n0\n0\n0\n0\n0\n0\n3\n5\n2\n2\n5\n5\n22\n\n\nTelegraph\n0\n0\n0\n0\n0\n0\n33\n14\n30\n35\n23\n9\n144\n\n\nTotal\n398\n348\n304\n295\n289\n283\n283\n256\n248\n201\n196\n107\n3208\n\n\n\n\n\nAlso, among articles that use person-first language, nearly half will also use condition-first language in the same article:\n\nCodeggvenn(list(\n  `Condition first` = condition_first_annotated$article_id, \n  `Person first` = person_first_annotated$article_id),\n  fill_color = c(\"white\", \"white\")) #+\n\n\n\nCode  #labs(title = \"Number of articles that use condition-first, person-first or both language types\")\nggsave(device = \"png\",\n       here::here(\"400_analysis\",\"venn_diagram_condition_person_first.png\"),\n       bg = \"white\", \n       width = 4,\n       height = 4)\nggvenn(list(\n  `Condition first` = condition_first_annotated$article_id, \n  `Person first` = person_first_annotated$article_id),\n  fill_color = c(\"#0073C2FF\", \"#CD534CFF\")) +\n  labs(title = \"Number of articles that use condition-first, person-first or both language types\")\n\n\n\n\nThis means that comparing the use of person-first and condition-first language using a Chi-square test will not be appropriate, as the same article will be counted towards both condition-first and person-first language.\nWe can, however, compare the number of articles that use either language type (i.e. ONLY condition-first and only person-first) by type of publication:\n\nCodelanguage_sourcetype_table <-\n  condition_first_annotated %>%\n  filter(!(article_id %in% person_first_annotated$article_id)) %>%\n  mutate(type = \"condition-first\") %>%\n  rbind({\n    person_first_annotated %>%\n    filter(!(article_id %in% condition_first_annotated$article_id)) %>%\n    mutate(type = \"person-first\")\n  }) %>%\n  group_by(source_type, type) %>%\n  count() %>%\n  pivot_wider(names_from = type, values_from = n) \nlanguage_sourcetype_table %>% \n  kable()\n\n\n\nsource_type\ncondition-first\nperson-first\n\n\n\nbroadsheet\n1141\n30\n\n\ntabloid\n2020\n29\n\n\n\n\n\nWe can see that person-first language is present in approximately the same number of articles in broadsheet and tabloid newspapers, whereas articles with only condition-first language are higher in number in tabloid publications.\n\nCodechisq_source_res <- chisq.test(language_sourcetype_table[,c(\"condition-first\", \"person-first\")])\nchisq_source_res\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\ndata:  language_sourcetype_table[, c(\"condition-first\", \"person-first\")]\nX-squared = 4.8274, df = 1, p-value = 0.02801\n\n\nThe Chi-square test results in a p-value that is less than 0.05, indicating a significant link between type of publication and type of language used.\nWhat is the effect size of this link?\n\nCodecramers_v <- function(df, chisq_result){\n  df_interest <- df[,c(\"condition-first\", \"person-first\")]\n  deg_f <- min(dim(df_interest)) - 1\n  sqrt(chisq_result$statistic / (sum(df_interest) * deg_f))\n}\ncramers_v(language_sourcetype_table, chisq_source_res)\n\n X-squared \n0.03871942 \n\n\nThe effect size is quite small (<0.2), indicating that while the result is statistically significant, the fields are only weakly associated.\nLet’s next use a similar approach to identify whether left- or right- leaning publications are different in their use of condition- vs person-first language. What is the total number of articles that use EITHER condition-first or person-first language by orientation of publication?\n\nCodelanguage_orientation_table <- condition_first_annotated %>%\n  filter(!(article_id %in% person_first_annotated$article_id)) %>%\n  mutate(type = \"condition-first\") %>%\n  rbind({\n    person_first_annotated %>%\n    filter(!(article_id %in% condition_first_annotated$article_id)) %>%\n    mutate(type = \"person-first\")\n  }) %>%\n  group_by(orientation, type) %>%\n  count() %>%\n  pivot_wider(names_from = type, values_from = n) \nlanguage_orientation_table %>% kable()\n\n\n\norientation\ncondition-first\nperson-first\n\n\n\nleft\n954\n26\n\n\nright\n2207\n33\n\n\n\n\n\nNext, let’s run a Chi-square test on this contingency table:\n\nCodechisq_language_orientation <- chisq.test(language_orientation_table[,c(\"condition-first\", \"person-first\")])\nchisq_language_orientation\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\ndata:  language_orientation_table[, c(\"condition-first\", \"person-first\")]\nX-squared = 4.6405, df = 1, p-value = 0.03123\n\n\nThe Chi-square test of independence is significant.\n\nCodecramers_v(language_orientation_table, chisq_language_orientation)\n\n X-squared \n0.03796227 \n\n\nHowever, the effect size is negligible (<= 0.2), indicating that once again the fields are only weakly associated.\n\nAs discussed above, the corpus contains articles that use condition-first, person-first and neither of these two language types. We can use repeated sampling of 1000 articles from the corpus 10000 times to explore how frequently we would observe articles from each of the three groups.\n\nCodesingle_row <- function(x) {\n  cbind(data.frame(\n    condition_first = sum(x %in% condition_first_annotated$article_id),\n        person_first = sum(x %in% person_first_annotated$article_id)))\n}\ndiff_boot <- purrr::map_dfr(\n  1:10000,\n  ~single_row(sample(metadata_full$article_id, 1000, replace = FALSE))\n  )\n\n\nWe can visualise the observed counts per 1000 articles from the 10000 resamples:\n\nCodediff_boot %>%\n  pivot_longer(cols = everything(),\n               names_to = \"language_type\", \n               values_to = \"count_per_10000_articles\") %>%\n  ggplot(aes(x = count_per_10000_articles,\n             fill = language_type)) +\n  geom_histogram(bins = 150) + theme(legend.position = \"bottom\") + \n  labs(\n    x = \"Count per 1000 articles sampled\",\n    y = \"Resamples with observed count\",\n    caption = \"Total 10 000 resamples of corpus with 1000 articles each\"\n  )\n\n\n\n\nWe can compare the mean of these two observed resamples:\nCodereport(t.test(diff_boot$person_first, diff_boot$condition_first))\nThe Welch Two Sample t-test testing the difference between diff_boot\\(person_first and diff_boot\\)condition_first (mean of x = 4.03, mean of y = 122.61) suggests that the effect is negative, statistically significant, and large (difference = -118.58, 95% CI [-118.78, -118.37], t(10724.90) = -1139.28, p < .001; Cohen’s d = -16.11, 95% CI [-16.33, -15.89])\nThis shows that on average of every 1000 articles sampled from the corpus, 4 will use person-first and 122 will use condition-first language.\n\nWe can also take a different approach, comparing the number of phrases that use each language type. Here each phrase will contribute only to one group. We do, however, need to confirm that most articles have a small number of phrases, vs a small number of articles with a large number of phrases fully underpinning our counts. Let’s look at how many articles have how many counts of each language usage:\n\nCodecount_language_types <- condition_first_annotated %>%\n  dplyr::select(article_id, no_hits_in_text) %>%\n  mutate(type=\"condition-first\") %>%\n  rbind({\n    person_first_annotated %>%\n      select(article_id, no_hits_in_text) %>%\n      mutate(type=\"person-first\")\n  })\ncount_language_types %>% \n  group_by(no_hits_in_text, type) %>%\n  count() %>%\n  pivot_wider(names_from=type, values_from = n, values_fill = 0) %>%\n  kable()\n\n\n\nno_hits_in_text\ncondition-first\nperson-first\n\n\n\n1\n2379\n93\n\n\n2\n508\n10\n\n\n3\n165\n1\n\n\n4\n82\n0\n\n\n5\n35\n0\n\n\n6\n19\n0\n\n\n7\n5\n0\n\n\n8\n11\n0\n\n\n10\n1\n2\n\n\n12\n2\n0\n\n\n13\n1\n0\n\n\n\n\nCodecount_language_types %>%\n  ggplot(aes(x = no_hits_in_text)) + geom_bar() + facet_grid(type~., scales = \"free_y\")\n\n\n\n\nMost articles have 1-2 uses of person-first/condition-first language, but some have up to 13 uses. How many total instances are there?\n\nCodecount_language_types %>% \n  group_by(type) %>% \n  summarise( instances = sum(no_hits_in_text),\n             articles = n()) %>%\n  kable()\n\n\n\ntype\ninstances\narticles\n\n\n\ncondition-first\n4677\n3208\n\n\nperson-first\n136\n106\n\n\n\n\n\n\nLet’s explore what the relative frequency of condition-first vs person-first language looks like.\n\nCodefreq_1 <- condition_first_annotated %>%\n  select(frequency) %>%\n  mutate(condition = \"Condition-first language\") %>%\n  rbind({\n  person_first_annotated %>%\n  select(frequency) %>%\n  mutate(condition = \"Person-first language\")\n  }) \nfreq_1_gt100words <- condition_first_annotated %>%\n  filter(wordcount_from_metatata >= 100) %>%\n  select(frequency) %>%\n  mutate(condition = \"Condition-first language\") %>%\n  rbind({\n  person_first_annotated %>%\n  filter(wordcount_from_metatata >= 100) %>%\n  select(frequency) %>%\n  mutate(condition = \"Person-first language\")\n  }) \nfreq_1 %>%\n  ggplot(aes(x = frequency, fill = condition)) +\n  facet_grid(condition~., scales = \"free_y\") +\n  geom_histogram(bins = 100) + \n  xlab(\"Frequency per thousand words\") + \n  ylab(\"Number of articles\") + theme(legend.position = \"none\") +\n  geom_vline(xintercept = 20, lty=2)\n\n\n\n\nLet’s create a boxplot to compare the frequency per thousand words:\n\nCodefreq_1 %>%\n  ggplot(aes( y =  frequency, x = condition)) + \n  geom_boxplot(outlier.shape = NA) +\n  scale_y_continuous(limits = quantile(freq_1$frequency, c(0.05, 0.95))) +\n  labs(\n    x = \"\",\n    y = \"Frequency per thousand words\",\n    title = \"Frequency of condition-first and person-first language in each article\"\n  )\n\n\n\n\nWe can then use a two-sample t-test to compare the mean frequency of condition-first vs person-first language in the corpus:\nCodecondition_first_frequencies <- freq_1 %>% \n  filter(condition == \"Condition-first language\") %>% \n  pull(frequency)\nperson_first_frequencies <- freq_1 %>%\n  filter(condition == \"Person-first language\") %>% \n  pull(frequency)\nreport(t.test(condition_first_frequencies,\n       person_first_frequencies\n       ))\nThe Welch Two Sample t-test testing the difference between condition_first_frequencies and person_first_frequencies (mean of x = 4.34, mean of y = 2.67) suggests that the effect is positive, statistically significant, and small (difference = 1.66, 95% CI [1.16, 2.17], t(131.59) = 6.49, p < .001; Cohen’s d = 0.44, 95% CI [0.30, 0.58])\nIn texts that use either condition-first, person-first languages or both, the frequency of condition-first language is higher (mean 4 words per 1000) than person-first (mean 2.7 words per 1000).\nThe below plot shows the article ids of articles with a word count less than 100 for person-first language, and article ids with word counts less than 100 where the frequency is greater than 20 for condition-first language\n\nCodecondition_first_annotated %>%\n  # select only texts less than 100 words\n  filter(wordcount_total <= 100) %>%\n  select(article_id, frequency) %>%\n  mutate(condition = \"Condition-first language\") %>%\n  # note that for condition-first only looking at those that are very high frequency here\n  filter(frequency >= 20) %>%\n  rbind({\n  person_first_annotated %>%\n  # select only texts less than 100 words\n  filter(wordcount_total <= 100) %>%\n  select(article_id, frequency) %>%\n  mutate(condition = \"Person-first language\")\n  }) %>% \n  group_by(frequency) %>%\n  mutate(cnt = n()) %>%\n  ggplot(aes(x = frequency, y = cnt, fill = condition, label = article_id)) +\n  facet_grid(condition~., scales = \"free_y\") +\n  geom_text_repel(check_overlap = TRUE, angle = 90) +\n  xlab(\"Frequency per thousand words\") + \n  ylab(\"Article ID & count\") + theme(legend.position = \"none\") +\n  geom_vline(xintercept = 20, lty=2)\n\n\n\n\nThere are a few texts with very high frequencies. These mostly occur in cases where the text length itself is quite short. We can consider whether we want to filter out texts with a word count of less than 100 words.\nIf we run a t-test on the dataset filtered to only contain texts greater than 100 words, we can see that while the results are still significant, the mean difference is less.\nCodecondition_first_frequencies_gt100 <- freq_1_gt100words %>% \n  filter(condition == \"Condition-first language\") %>% \n  pull(frequency)\nperson_first_frequencies_gt100 <- freq_1_gt100words %>%\n  filter(condition == \"Person-first language\") %>% \n  pull(frequency)\nreport(t.test(\n  condition_first_frequencies_gt100,\n  person_first_frequencies_gt100\n       ))\nThe Welch Two Sample t-test testing the difference between condition_first_frequencies_gt100 and person_first_frequencies_gt100 (mean of x = 3.69, mean of y = 2.60) suggests that the effect is positive, statistically significant, and small (difference = 1.10, 95% CI [0.62, 1.57], t(118.21) = 4.58, p < .001; Cohen’s d = 0.38, 95% CI [0.21, 0.55])\n\nLet’s visualise the frequency of person-first language by publication:\n\nCodeperson_first_annotated %>%\n  select(source, frequency, year, source_type) %>%\n  ggplot(aes(x = reorder(source, frequency), \n             y = frequency, \n             fill = source_type)) +\n  geom_boxplot(outlier.shape = NA) + \n  theme(axis.text.x=element_text(angle = 45, hjust =1),\n        legend.position = \"bottom\") +\n  labs(x = NULL, \n       y = \"Frequency per thousand words\") +\n  geom_jitter(width = 0.25, alpha = 0.5) \n\n\n\n\nAnd per year:\n\nCodeperson_first_annotated %>%\n  select(source, frequency, year, source_type) %>%\n  ggplot(aes(x = as.factor(year), y = frequency, \n             fill = source_type,  shape = source_type)) +\n  facet_wrap(~source_type) +\n  geom_boxplot(outlier.shape = NA) + theme_bw() +\n  theme(axis.text.x=element_text(angle = 45, hjust =1),\n        legend.position = \"bottom\") +\n  labs(x = NULL, y = \"Frequency per thousand words\") +\n  geom_jitter(width = 0.1, alpha = 0.5) \n\n\n\n\n\nWe can also look at the frequency of person-first and condition-first language by dividing the number of observations of each language type by the total word count of articles in which they are found (i.e. frequency per 1000 words not on a per-article basis, but on a per total word count of articles in the group).\nWe can look at the frequency of person first language\n\nCodefreq_per_subcorpus <- function(df, group_vars){\n  df %>%\n  group_by(!!!group_vars) %>%\n  summarise(\n    total_instances = sum(no_hits_in_text),\n    total_wc = sum(wordcount_total),\n    instances_per_1000 = 1000 * total_instances/total_wc\n  )\n}\ninner_join(\n  {freq_per_subcorpus(person_first_annotated, \n                      group_vars=vars(year)) %>%\n      rename(\"instances_person_first\" = \"total_instances\",\n             \"wc_personfirst\" = \"total_wc\",\n             \"person first instances per 1000 words\" = \"instances_per_1000\")\n  },\n  {freq_per_subcorpus(\n    condition_first_annotated, \n    group_vars=vars(year)) %>%\n      rename(\"instances_cond_first\" = \"total_instances\",\n             \"wc_condfirst\" = \"total_wc\",\n             \"condition first instances per 1000 words\" = \"instances_per_1000\")\n  }) %>% kable(digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\nyear\ninstances_person_first\nwc_personfirst\nperson first instances per 1000 words\ninstances_cond_first\nwc_condfirst\ncondition first instances per 1000 words\n\n\n\n2008\n6\n4386\n1.37\n553\n225124\n2.46\n\n\n2009\n7\n4319\n1.62\n507\n187891\n2.70\n\n\n2010\n4\n2104\n1.90\n452\n170920\n2.64\n\n\n2011\n5\n4144\n1.21\n447\n152357\n2.93\n\n\n2012\n5\n2820\n1.77\n385\n170630\n2.26\n\n\n2013\n12\n7251\n1.65\n382\n156399\n2.44\n\n\n2014\n9\n3322\n2.71\n408\n155966\n2.62\n\n\n2015\n9\n5223\n1.72\n436\n145468\n3.00\n\n\n2016\n15\n8846\n1.70\n388\n145091\n2.67\n\n\n2017\n18\n10394\n1.73\n309\n104202\n2.97\n\n\n2018\n9\n3460\n2.60\n273\n145452\n1.88\n\n\n2019\n37\n10379\n3.56\n137\n59344\n2.31\n\n\n\n\nCoderbind({freq_per_subcorpus(person_first_annotated, \n                   group_vars=vars(year)) %>%\n    mutate(language = \"Person-first\")},\n      {freq_per_subcorpus(condition_first_annotated, \n                   group_vars=vars(year)) %>%\n    mutate(language = \"Condition-first\")}\n        ) %>%\n  ggplot(aes(x = as.factor(year),\n             y = instances_per_1000,\n             col = language)) +\n  geom_point() +\n  labs(x = \"Year\",\n       y = \"Frequency per 1000 words\",\n       title = \"Person and condition-first language by year\",\n       caption = \"Instances per 1000 words across all articles that use language type\")\n\n\n\n\nWe can also look at this across sources:\n\nCodeinner_join(\n  {freq_per_subcorpus(person_first_annotated, \n                      group_vars=vars(source)) %>%\n      rename(\"instances_person_first\" = \"total_instances\",\n             \"wc_personfirst\" = \"total_wc\",\n             \"person first instances per 1000 words\" = \"instances_per_1000\")\n  },\n  {freq_per_subcorpus(\n    condition_first_annotated, \n    group_vars=vars(source)) %>%\n      rename(\"instances_cond_first\" = \"total_instances\",\n             \"wc_condfirst\" = \"total_wc\",\n             \"condition first instances per 1000 words\" = \"instances_per_1000\")\n  }) %>% kable(digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\nsource\ninstances_person_first\nwc_personfirst\nperson first instances per 1000 words\ninstances_cond_first\nwc_condfirst\ncondition first instances per 1000 words\n\n\n\nAdvertiser\n9\n3655\n2.46\n636\n215470\n2.95\n\n\nAge\n21\n13586\n1.55\n509\n228541\n2.23\n\n\nAustralian\n10\n6368\n1.57\n283\n163862\n1.73\n\n\nBrisTimes\n1\n1101\n0.91\n35\n16760\n2.09\n\n\nCanTimes\n17\n4774\n3.56\n342\n130561\n2.62\n\n\nCourierMail\n12\n5018\n2.39\n607\n236038\n2.57\n\n\nHeraldSun\n16\n8030\n1.99\n705\n239849\n2.94\n\n\nHobMercury\n2\n673\n2.97\n240\n68552\n3.50\n\n\nNorthernT\n2\n521\n3.84\n120\n30149\n3.98\n\n\nSydHerald\n37\n18111\n2.04\n697\n319498\n2.18\n\n\nTelegraph\n6\n3636\n1.65\n186\n70524\n2.64\n\n\nWestAus\n3\n1175\n2.55\n317\n99040\n3.20\n\n\n\n\nCoderbind({freq_per_subcorpus(person_first_annotated, \n                   group_vars=vars(source)) %>%\n    mutate(language = \"Person-first\")},\n      {freq_per_subcorpus(condition_first_annotated, \n                   group_vars=vars(source)) %>%\n    mutate(language = \"Condition-first\")}\n        ) %>%\n  ggplot(aes(x = source,\n             y = instances_per_1000,\n             col = language)) +\n  geom_point() +\n  labs(x = \"Source\",\n       y = \"Frequency per 1000 words\",\n       title = \"Person and condition-first language by source\",\n       caption = \"Instances per 1000 words across all sources that use language type\") +\n  theme(axis.text.x = element_text(angle=90))\n\n\n\n\nWe can also do this across source and year:\n\nCodeinner_join(\n  {freq_per_subcorpus(person_first_annotated, \n                      group_vars=vars(source, year)) %>%\n      rename(\"instances_person_first\" = \"total_instances\",\n             \"wc_personfirst\" = \"total_wc\",\n             \"person first instances per 1000 words\" = \"instances_per_1000\")\n  },\n  {freq_per_subcorpus(\n    condition_first_annotated, \n    group_vars=vars(source, year)) %>%\n      rename(\"instances_cond_first\" = \"total_instances\",\n             \"wc_condfirst\" = \"total_wc\",\n             \"condition first instances per 1000 words\" = \"instances_per_1000\")\n  }) %>% kable(digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nsource\nyear\ninstances_person_first\nwc_personfirst\nperson first instances per 1000 words\ninstances_cond_first\nwc_condfirst\ncondition first instances per 1000 words\n\n\n\nAdvertiser\n2008\n1\n243\n4.12\n49\n13109\n3.74\n\n\nAdvertiser\n2009\n1\n159\n6.29\n69\n22082\n3.12\n\n\nAdvertiser\n2012\n1\n462\n2.16\n58\n20250\n2.86\n\n\nAdvertiser\n2013\n1\n413\n2.42\n67\n22586\n2.97\n\n\nAdvertiser\n2016\n1\n561\n1.78\n52\n19254\n2.70\n\n\nAdvertiser\n2017\n3\n1571\n1.91\n40\n12681\n3.15\n\n\nAdvertiser\n2019\n1\n246\n4.07\n21\n7529\n2.79\n\n\nAge\n2008\n2\n1316\n1.52\n68\n36713\n1.85\n\n\nAge\n2010\n1\n487\n2.05\n44\n13081\n3.36\n\n\nAge\n2011\n2\n2630\n0.76\n47\n13058\n3.60\n\n\nAge\n2012\n2\n953\n2.10\n49\n27123\n1.81\n\n\nAge\n2013\n1\n1319\n0.76\n34\n21790\n1.56\n\n\nAge\n2014\n2\n935\n2.14\n33\n14387\n2.29\n\n\nAge\n2015\n3\n2257\n1.33\n79\n25206\n3.13\n\n\nAge\n2016\n2\n1721\n1.16\n48\n20528\n2.34\n\n\nAge\n2017\n2\n973\n2.06\n26\n14002\n1.86\n\n\nAge\n2018\n2\n465\n4.30\n31\n20118\n1.54\n\n\nAge\n2019\n2\n530\n3.77\n10\n6019\n1.66\n\n\nAustralian\n2009\n2\n1289\n1.55\n50\n22444\n2.23\n\n\nAustralian\n2013\n1\n361\n2.77\n13\n7129\n1.82\n\n\nAustralian\n2016\n1\n758\n1.32\n15\n9868\n1.52\n\n\nAustralian\n2017\n3\n1188\n2.53\n9\n5008\n1.80\n\n\nAustralian\n2018\n1\n1357\n0.74\n14\n11260\n1.24\n\n\nAustralian\n2019\n2\n1415\n1.41\n11\n6369\n1.73\n\n\nBrisTimes\n2016\n1\n1101\n0.91\n3\n1661\n1.81\n\n\nCanTimes\n2009\n1\n246\n4.07\n23\n8318\n2.77\n\n\nCanTimes\n2012\n1\n539\n1.86\n32\n14363\n2.23\n\n\nCanTimes\n2013\n2\n1764\n1.13\n29\n10193\n2.85\n\n\nCanTimes\n2015\n1\n573\n1.75\n48\n12869\n3.73\n\n\nCanTimes\n2017\n1\n520\n1.92\n21\n6585\n3.19\n\n\nCanTimes\n2019\n11\n1132\n9.72\n8\n2428\n3.29\n\n\nCourierMail\n2008\n1\n580\n1.72\n92\n44008\n2.09\n\n\nCourierMail\n2010\n1\n543\n1.84\n72\n29876\n2.41\n\n\nCourierMail\n2013\n1\n425\n2.35\n60\n24932\n2.41\n\n\nCourierMail\n2014\n2\n550\n3.64\n51\n13082\n3.90\n\n\nCourierMail\n2015\n1\n293\n3.41\n50\n15976\n3.13\n\n\nCourierMail\n2016\n4\n1292\n3.10\n41\n10375\n3.95\n\n\nCourierMail\n2017\n1\n1077\n0.93\n44\n11410\n3.86\n\n\nCourierMail\n2018\n1\n258\n3.88\n38\n18314\n2.07\n\n\nHeraldSun\n2008\n1\n1815\n0.55\n90\n31663\n2.84\n\n\nHeraldSun\n2011\n2\n946\n2.11\n94\n30705\n3.06\n\n\nHeraldSun\n2013\n4\n975\n4.10\n62\n22042\n2.81\n\n\nHeraldSun\n2014\n2\n671\n2.98\n52\n16593\n3.13\n\n\nHeraldSun\n2016\n2\n1049\n1.91\n46\n17646\n2.61\n\n\nHeraldSun\n2017\n1\n1079\n0.93\n43\n13259\n3.24\n\n\nHeraldSun\n2018\n1\n358\n2.79\n31\n10046\n3.09\n\n\nHeraldSun\n2019\n3\n1137\n2.64\n14\n4331\n3.23\n\n\nHobMercury\n2009\n1\n405\n2.47\n23\n7858\n2.93\n\n\nHobMercury\n2017\n1\n268\n3.73\n9\n2758\n3.26\n\n\nNorthernT\n2014\n1\n92\n10.87\n11\n1968\n5.59\n\n\nNorthernT\n2015\n1\n429\n2.33\n9\n1351\n6.66\n\n\nSydHerald\n2008\n1\n432\n2.31\n71\n29391\n2.42\n\n\nSydHerald\n2009\n2\n2220\n0.90\n90\n34411\n2.62\n\n\nSydHerald\n2010\n1\n586\n1.71\n66\n30403\n2.17\n\n\nSydHerald\n2011\n1\n568\n1.76\n61\n23340\n2.61\n\n\nSydHerald\n2012\n1\n866\n1.15\n56\n32263\n1.74\n\n\nSydHerald\n2013\n2\n1994\n1.00\n52\n29797\n1.75\n\n\nSydHerald\n2014\n1\n622\n1.61\n45\n24530\n1.83\n\n\nSydHerald\n2015\n3\n1671\n1.80\n80\n26517\n3.02\n\n\nSydHerald\n2016\n1\n1101\n0.91\n68\n26221\n2.59\n\n\nSydHerald\n2017\n3\n1340\n2.24\n37\n16969\n2.18\n\n\nSydHerald\n2018\n3\n792\n3.79\n46\n31308\n1.47\n\n\nSydHerald\n2019\n18\n5919\n3.04\n25\n14348\n1.74\n\n\nTelegraph\n2014\n1\n452\n2.21\n45\n17477\n2.57\n\n\nTelegraph\n2016\n2\n967\n2.07\n36\n13829\n2.60\n\n\nTelegraph\n2017\n2\n1987\n1.01\n54\n13954\n3.87\n\n\nTelegraph\n2018\n1\n230\n4.35\n27\n13466\n2.01\n\n\nWestAus\n2010\n1\n488\n2.05\n39\n10600\n3.68\n\n\nWestAus\n2016\n1\n296\n3.38\n31\n11382\n2.72\n\n\nWestAus\n2017\n1\n391\n2.56\n16\n3727\n4.29\n\n\n\n\nCoderbind({freq_per_subcorpus(person_first_annotated, \n                   group_vars=vars(source, year)) %>%\n    mutate(language = \"Person-first\")},\n      {freq_per_subcorpus(condition_first_annotated, \n                   group_vars=vars(source, year)) %>%\n    mutate(language = \"Condition-first\")}\n        ) %>%\n  ggplot(aes(x = as.factor(year),\n             y = instances_per_1000,\n             col = language)) +\n  geom_jitter() +\n  facet_wrap(~source) +\n  labs(x = \"Year\",\n       y = \"Frequency per 1000 words\",\n       title = \"Person and condition-first language by source and year\",\n       caption = \"Instances per 1000 words across all sources & years that use language type\") +\n  theme(axis.text.x = element_text(angle=90))\n\n\n\n\nNote that above we considered the word count ONLY in articles that featured that particular language type in the denominator Next, we conduct the same analysis, but including all articles from that particular source, year or both (irrespective of whether they feature the language type).\n\nCodefreq_entire_corpus <- function(df, group_vars){\n  df %>%\n  group_by(!!!group_vars) %>%\n  summarise(\n    person_first_instances = sum(person_first),\n    condition_first_instances = sum(condition_first),\n    total_wc = sum(wordcount_total),\n    person_first_per_1000 = 1000 * person_first_instances/total_wc,\n    cond_first_per_1000 = 1000 * condition_first_instances/total_wc\n  )\n}\ncondition_person_comparison_together <- \n  full_join(\n    full_join(\n      {person_first_annotated %>%\n          select(article_id, no_hits_in_text) %>%\n          rename(person_first = no_hits_in_text)},\n      {condition_first_annotated %>%\n          select(article_id, no_hits_in_text) %>%\n          rename(condition_first = no_hits_in_text)}\n    ),\n    {metadata_full %>%\n        select(article_id, source, year, wordcount_total)}) %>%\n  # fill NAs with 0\n  mutate_if(is.numeric,coalesce,0) \n  # now generate the instances\nfreq_entire_corpus(condition_person_comparison_together, vars(year)) %>% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\nyear\nperson_first_instances\ncondition_first_instances\ntotal_wc\nperson_first_per_1000\ncond_first_per_1000\n\n\n\n2008\n6\n553\n1863018\n0.0032206\n0.2968302\n\n\n2009\n7\n507\n1524207\n0.0045926\n0.3326320\n\n\n2010\n4\n452\n1454853\n0.0027494\n0.3106843\n\n\n2011\n5\n447\n1318579\n0.0037920\n0.3390013\n\n\n2012\n5\n385\n1280045\n0.0039061\n0.3007707\n\n\n2013\n12\n382\n1625785\n0.0073810\n0.2349634\n\n\n2014\n9\n408\n1271294\n0.0070794\n0.3209328\n\n\n2015\n9\n436\n1596597\n0.0056370\n0.2730808\n\n\n2016\n15\n388\n1156387\n0.0129714\n0.3355278\n\n\n2017\n18\n309\n1121553\n0.0160492\n0.2755108\n\n\n2018\n9\n273\n1211129\n0.0074311\n0.2254095\n\n\n2019\n37\n137\n1012594\n0.0365398\n0.1352961\n\n\n\n\nCodefreq_entire_corpus(condition_person_comparison_together, vars(year)) %>%\n  select(year, ends_with(\"per_1000\")) %>%\n  pivot_longer(cols = ends_with(\"1000\"), names_to = \"type\", values_to = \"value\") %>%\n  mutate(type = stringr::str_replace_all(type, \"_per_1000\", \"\"),\n         type = case_when(type == \"cond_first\" ~ \"Condition-first\", TRUE ~ \"Person-first\")) %>%\n  ggplot(aes(x = as.factor(year), y= value, col = type)) +\n  geom_point() + \n  labs(x = \"Year\",\n       y = \"Instances per 1000 words in corpus by year\",\n       col = \"\")\n\n\n\n\nWe can see that if we consider the entire corpus, instances of condition-first language seem to be somewhat lower by year in 2017 onwards. Let’s look at the numbers by source:\n\nCodefreq_entire_corpus({condition_person_comparison_together %>% filter(!(source %in% c(\"Telegraph\", \"BrisTimes\")))}, vars(source)) %>% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\nsource\nperson_first_instances\ncondition_first_instances\ntotal_wc\nperson_first_per_1000\ncond_first_per_1000\n\n\n\nAdvertiser\n9\n636\n1750948\n0.0051401\n0.3632318\n\n\nAge\n21\n509\n2399436\n0.0087521\n0.2121332\n\n\nAustralian\n10\n283\n1722950\n0.0058040\n0.1642532\n\n\nCanTimes\n17\n342\n1431478\n0.0118758\n0.2389139\n\n\nCourierMail\n12\n607\n1684101\n0.0071255\n0.3604297\n\n\nHeraldSun\n16\n705\n1867259\n0.0085687\n0.3775588\n\n\nHobMercury\n2\n240\n690782\n0.0028953\n0.3474323\n\n\nNorthernT\n2\n120\n301923\n0.0066242\n0.3974523\n\n\nSydHerald\n37\n697\n2901109\n0.0127537\n0.2402530\n\n\nWestAus\n3\n317\n892280\n0.0033622\n0.3552696\n\n\n\n\nCodefreq_entire_corpus({\n  condition_person_comparison_together %>% filter(!(source %in% c(\"Telegraph\", \"BrisTimes\")))\n}, vars(source)) %>%\n  select(source, ends_with(\"per_1000\")) %>%\n  pivot_longer(cols = ends_with(\"1000\"), names_to = \"type\", values_to = \"value\") %>%\n  mutate(type = stringr::str_replace_all(type, \"_per_1000\", \"\"),\n         type = case_when(type == \"cond_first\" ~ \"Condition-first\", TRUE ~ \"Person-first\")) %>%\n  ggplot(aes(x = source, y= value, col = type)) +\n  geom_point() + \n  labs(x = \"Source\",\n       y = \"Instances per 1000 words in corpus by source\",\n       col = \"\") +\n  theme(axis.text.x = element_text(angle=90))\n\n\n\n\nWe can see that usage of condition-first language is quite varied by source.\nLet’s look at source and year simultaneously:\n\nCodefreq_entire_corpus({condition_person_comparison_together %>% filter(!(source %in% c(\"Telegraph\", \"BrisTimes\")))}, vars(source, year)) %>% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\nsource\nyear\nperson_first_instances\ncondition_first_instances\ntotal_wc\nperson_first_per_1000\ncond_first_per_1000\n\n\n\nAdvertiser\n2008\n1\n49\n152154\n0.0065723\n0.3220421\n\n\nAdvertiser\n2009\n1\n69\n158395\n0.0063133\n0.4356198\n\n\nAdvertiser\n2010\n0\n58\n155561\n0.0000000\n0.3728441\n\n\nAdvertiser\n2011\n0\n67\n151211\n0.0000000\n0.4430895\n\n\nAdvertiser\n2012\n1\n58\n162157\n0.0061669\n0.3576781\n\n\nAdvertiser\n2013\n1\n67\n207306\n0.0048238\n0.3231937\n\n\nAdvertiser\n2014\n0\n63\n151973\n0.0000000\n0.4145473\n\n\nAdvertiser\n2015\n0\n62\n173493\n0.0000000\n0.3573631\n\n\nAdvertiser\n2016\n1\n52\n126129\n0.0079284\n0.4122763\n\n\nAdvertiser\n2017\n3\n40\n117002\n0.0256406\n0.3418745\n\n\nAdvertiser\n2018\n0\n30\n100696\n0.0000000\n0.2979264\n\n\nAdvertiser\n2019\n1\n21\n94871\n0.0105406\n0.2213532\n\n\nAge\n2008\n2\n68\n316472\n0.0063197\n0.2148689\n\n\nAge\n2009\n0\n40\n201930\n0.0000000\n0.1980884\n\n\nAge\n2010\n1\n44\n181424\n0.0055119\n0.2425258\n\n\nAge\n2011\n2\n47\n167790\n0.0119197\n0.2801120\n\n\nAge\n2012\n2\n49\n163014\n0.0122689\n0.3005877\n\n\nAge\n2013\n1\n34\n269995\n0.0037038\n0.1259283\n\n\nAge\n2014\n2\n33\n172156\n0.0116174\n0.1916866\n\n\nAge\n2015\n3\n79\n283729\n0.0105735\n0.2784347\n\n\nAge\n2016\n2\n48\n197653\n0.0101187\n0.2428498\n\n\nAge\n2017\n2\n26\n157039\n0.0127357\n0.1655640\n\n\nAge\n2018\n2\n31\n161020\n0.0124208\n0.1925227\n\n\nAge\n2019\n2\n10\n127214\n0.0157215\n0.0786077\n\n\nAustralian\n2008\n0\n59\n258517\n0.0000000\n0.2282248\n\n\nAustralian\n2009\n2\n50\n188289\n0.0106220\n0.2655492\n\n\nAustralian\n2010\n0\n27\n154723\n0.0000000\n0.1745054\n\n\nAustralian\n2011\n0\n30\n166076\n0.0000000\n0.1806402\n\n\nAustralian\n2012\n0\n22\n142596\n0.0000000\n0.1542820\n\n\nAustralian\n2013\n1\n13\n132956\n0.0075213\n0.0977767\n\n\nAustralian\n2014\n0\n24\n130256\n0.0000000\n0.1842525\n\n\nAustralian\n2015\n0\n9\n116308\n0.0000000\n0.0773807\n\n\nAustralian\n2016\n1\n15\n91995\n0.0108702\n0.1630523\n\n\nAustralian\n2017\n3\n9\n96918\n0.0309540\n0.0928620\n\n\nAustralian\n2018\n1\n14\n127452\n0.0078461\n0.1098453\n\n\nAustralian\n2019\n2\n11\n116864\n0.0171139\n0.0941265\n\n\nCanTimes\n2008\n0\n20\n111564\n0.0000000\n0.1792693\n\n\nCanTimes\n2009\n1\n23\n119572\n0.0083632\n0.1923527\n\n\nCanTimes\n2010\n0\n24\n106258\n0.0000000\n0.2258653\n\n\nCanTimes\n2011\n0\n36\n114314\n0.0000000\n0.3149221\n\n\nCanTimes\n2012\n1\n32\n105464\n0.0094819\n0.3034211\n\n\nCanTimes\n2013\n2\n29\n175309\n0.0114084\n0.1654222\n\n\nCanTimes\n2014\n0\n46\n145337\n0.0000000\n0.3165058\n\n\nCanTimes\n2015\n1\n48\n162947\n0.0061370\n0.2945743\n\n\nCanTimes\n2016\n0\n32\n77167\n0.0000000\n0.4146850\n\n\nCanTimes\n2017\n1\n21\n119117\n0.0083951\n0.1762973\n\n\nCanTimes\n2018\n0\n23\n135017\n0.0000000\n0.1703489\n\n\nCanTimes\n2019\n11\n8\n59412\n0.1851478\n0.1346529\n\n\nCourierMail\n2008\n1\n92\n249904\n0.0040015\n0.3681414\n\n\nCourierMail\n2009\n0\n59\n228541\n0.0000000\n0.2581594\n\n\nCourierMail\n2010\n1\n72\n191892\n0.0052113\n0.3752111\n\n\nCourierMail\n2011\n0\n43\n131489\n0.0000000\n0.3270236\n\n\nCourierMail\n2012\n0\n32\n124784\n0.0000000\n0.2564431\n\n\nCourierMail\n2013\n1\n60\n165817\n0.0060307\n0.3618447\n\n\nCourierMail\n2014\n2\n51\n100723\n0.0198564\n0.5063392\n\n\nCourierMail\n2015\n1\n50\n110106\n0.0090822\n0.4541079\n\n\nCourierMail\n2016\n4\n41\n71961\n0.0555857\n0.5697531\n\n\nCourierMail\n2017\n1\n44\n93348\n0.0107126\n0.4713545\n\n\nCourierMail\n2018\n1\n38\n114008\n0.0087713\n0.3333099\n\n\nCourierMail\n2019\n0\n25\n101528\n0.0000000\n0.2462375\n\n\nHeraldSun\n2008\n1\n90\n244227\n0.0040946\n0.3685096\n\n\nHeraldSun\n2009\n0\n91\n191536\n0.0000000\n0.4751065\n\n\nHeraldSun\n2010\n0\n82\n195937\n0.0000000\n0.4185019\n\n\nHeraldSun\n2011\n2\n94\n188824\n0.0105919\n0.4978181\n\n\nHeraldSun\n2012\n0\n65\n162385\n0.0000000\n0.4002833\n\n\nHeraldSun\n2013\n4\n62\n202876\n0.0197165\n0.3056054\n\n\nHeraldSun\n2014\n2\n52\n126605\n0.0157972\n0.4107263\n\n\nHeraldSun\n2015\n0\n35\n142687\n0.0000000\n0.2452921\n\n\nHeraldSun\n2016\n2\n46\n100735\n0.0198541\n0.4566437\n\n\nHeraldSun\n2017\n1\n43\n104785\n0.0095434\n0.4103641\n\n\nHeraldSun\n2018\n1\n31\n104080\n0.0096080\n0.2978478\n\n\nHeraldSun\n2019\n3\n14\n102582\n0.0292449\n0.1364762\n\n\nHobMercury\n2008\n0\n45\n93465\n0.0000000\n0.4814636\n\n\nHobMercury\n2009\n1\n23\n68016\n0.0147024\n0.3381557\n\n\nHobMercury\n2010\n0\n23\n72280\n0.0000000\n0.3182070\n\n\nHobMercury\n2011\n0\n26\n47973\n0.0000000\n0.5419715\n\n\nHobMercury\n2012\n0\n19\n60444\n0.0000000\n0.3143405\n\n\nHobMercury\n2013\n0\n31\n87371\n0.0000000\n0.3548088\n\n\nHobMercury\n2014\n0\n12\n29587\n0.0000000\n0.4055835\n\n\nHobMercury\n2015\n0\n24\n33753\n0.0000000\n0.7110479\n\n\nHobMercury\n2016\n0\n10\n60253\n0.0000000\n0.1659668\n\n\nHobMercury\n2017\n1\n9\n46038\n0.0217212\n0.1954907\n\n\nHobMercury\n2018\n0\n13\n50759\n0.0000000\n0.2561122\n\n\nHobMercury\n2019\n0\n5\n40843\n0.0000000\n0.1224200\n\n\nNorthernT\n2008\n0\n12\n34752\n0.0000000\n0.3453039\n\n\nNorthernT\n2009\n0\n27\n33503\n0.0000000\n0.8058980\n\n\nNorthernT\n2010\n0\n17\n32606\n0.0000000\n0.5213764\n\n\nNorthernT\n2011\n0\n6\n21798\n0.0000000\n0.2752546\n\n\nNorthernT\n2012\n0\n10\n21011\n0.0000000\n0.4759412\n\n\nNorthernT\n2013\n0\n10\n26708\n0.0000000\n0.3744196\n\n\nNorthernT\n2014\n1\n11\n20290\n0.0492854\n0.5421390\n\n\nNorthernT\n2015\n1\n9\n26207\n0.0381577\n0.3434197\n\n\nNorthernT\n2016\n0\n6\n29146\n0.0000000\n0.2058602\n\n\nNorthernT\n2017\n0\n8\n22253\n0.0000000\n0.3595021\n\n\nNorthernT\n2018\n0\n3\n16032\n0.0000000\n0.1871257\n\n\nNorthernT\n2019\n0\n1\n17617\n0.0000000\n0.0567634\n\n\nSydHerald\n2008\n1\n71\n263075\n0.0038012\n0.2698850\n\n\nSydHerald\n2009\n2\n90\n232899\n0.0085874\n0.3864336\n\n\nSydHerald\n2010\n1\n66\n271396\n0.0036847\n0.2431871\n\n\nSydHerald\n2011\n1\n61\n247334\n0.0040431\n0.2466301\n\n\nSydHerald\n2012\n1\n56\n262970\n0.0038027\n0.2129520\n\n\nSydHerald\n2013\n2\n52\n256756\n0.0077895\n0.2025269\n\n\nSydHerald\n2014\n1\n45\n197915\n0.0050527\n0.2273703\n\n\nSydHerald\n2015\n3\n80\n325532\n0.0092157\n0.2457516\n\n\nSydHerald\n2016\n1\n68\n224438\n0.0044556\n0.3029790\n\n\nSydHerald\n2017\n3\n37\n191646\n0.0156539\n0.1930643\n\n\nSydHerald\n2018\n3\n46\n219004\n0.0136984\n0.2100418\n\n\nSydHerald\n2019\n18\n25\n208144\n0.0864786\n0.1201092\n\n\nWestAus\n2008\n0\n47\n138888\n0.0000000\n0.3384022\n\n\nWestAus\n2009\n0\n35\n101526\n0.0000000\n0.3447393\n\n\nWestAus\n2010\n1\n39\n92776\n0.0107786\n0.4203673\n\n\nWestAus\n2011\n0\n37\n81770\n0.0000000\n0.4524887\n\n\nWestAus\n2012\n0\n42\n75220\n0.0000000\n0.5583621\n\n\nWestAus\n2013\n0\n24\n97018\n0.0000000\n0.2473768\n\n\nWestAus\n2014\n0\n21\n96631\n0.0000000\n0.2173216\n\n\nWestAus\n2015\n0\n17\n69706\n0.0000000\n0.2438814\n\n\nWestAus\n2016\n1\n31\n67430\n0.0148302\n0.4597360\n\n\nWestAus\n2017\n1\n16\n45643\n0.0219092\n0.3505466\n\n\nWestAus\n2018\n0\n7\n18468\n0.0000000\n0.3790340\n\n\nWestAus\n2019\n0\n1\n7204\n0.0000000\n0.1388118\n\n\n\n\nCodefreq_entire_corpus({\n  condition_person_comparison_together %>% filter(!(source %in% c(\"Telegraph\", \"BrisTimes\")))\n}, vars(source, year)) %>%\n  select(source, year, ends_with(\"per_1000\")) %>%\n  pivot_longer(cols = ends_with(\"1000\"), names_to = \"type\", values_to = \"value\") %>%\n  mutate(type = stringr::str_replace_all(type, \"_per_1000\", \"\"),\n         type = case_when(type == \"cond_first\" ~ \"Condition-first\", TRUE ~ \"Person-first\")) %>%\n  ggplot(aes(x = as.factor(year), y= value, col = type)) +\n  geom_point() + \n  facet_wrap(~source) +\n  labs(x = \"Year\",\n       y = \"Instances per 1000 words in corpus by source & year\",\n       col = \"\") +\n  theme(axis.text.x = element_text(angle=90))\n\n\n\n\n\nAs we discussed, we have sufficient data to explore the use of condition-first language across time and by type of publication, except for the Brisbane Times and Daily Telegraph, for which we are missing data from 2008-2013:\n\nCodeassess_year_source(condition_first_annotated)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsource\n2008\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\nTotal\n\n\n\nAdvertiser\n40\n53\n43\n49\n40\n51\n43\n41\n37\n25\n19\n15\n456\n\n\nAge\n49\n24\n24\n28\n37\n25\n22\n34\n26\n18\n19\n9\n315\n\n\nAustralian\n35\n26\n20\n22\n17\n11\n16\n8\n9\n7\n12\n8\n191\n\n\nCanTimes\n15\n18\n14\n18\n24\n17\n31\n24\n18\n10\n19\n4\n212\n\n\nCourierMail\n65\n44\n50\n31\n25\n40\n38\n36\n29\n29\n28\n19\n434\n\n\nHeraldSun\n66\n71\n56\n61\n51\n49\n35\n25\n32\n28\n22\n13\n509\n\n\nHobMercury\n31\n16\n15\n20\n14\n23\n8\n13\n8\n8\n11\n5\n172\n\n\nNorthernT\n12\n21\n13\n5\n7\n10\n7\n5\n4\n7\n3\n1\n95\n\n\nSydHerald\n47\n52\n41\n35\n38\n41\n30\n37\n36\n23\n32\n18\n430\n\n\nWestAus\n38\n23\n28\n26\n36\n16\n17\n14\n17\n9\n3\n1\n228\n\n\nBrisTimes\n0\n0\n0\n0\n0\n0\n3\n5\n2\n2\n5\n5\n22\n\n\nTelegraph\n0\n0\n0\n0\n0\n0\n33\n14\n30\n35\n23\n9\n144\n\n\nTotal\n398\n348\n304\n295\n289\n283\n283\n256\n248\n201\n196\n107\n3208\n\n\n\n\nCodecondition_first_annotated_for_modelling <- condition_first_annotated %>%\n  filter( !(source %in% c(\"BrisTimes\", \"Telegraph\")))\n\n\nLet’s look at the number of articles by publication and year. We can see that this number is declining; however, this is likely to be attributable to the overall decline in the number of articles featuring obes*, as discussed in the EDA.\n\nCodecondition_first_annotated %>%\n  select(year, source, source_type) %>%\n  group_by(year, source) %>%\n  count() %>%\n  ggplot(aes(x = year, y = n, col = source)) + \n  geom_line() +\n  labs(x = \"\", y = \"Number of articles\") +\n  theme(axis.text.x = element_text(angle = 90),\n        legend.position = \"NA\") +\n  scale_x_continuous(breaks = unique(condition_first_annotated$year)) + \n  facet_wrap(~source)\n\n\n\n\n\nThe normalised frequency is distributed log-normally across all texts:\n\nCodecondition_first_annotated %>%\n  select(frequency) %>%\n  ggplot(aes(x = log(frequency))) + \n  geom_histogram(bins = 75)\n\n\n\n\nLet’s look at the difference in frequency across time (only the variability of which should be sensitive to the number of articles per year, not the absolute values):\nWe can start by using a jitter plot:\n\nCodecondition_first_annotated %>%\n  ggplot(aes(x = as.factor(year), \n             y = log(frequency), \n             fill = year)) + \n  geom_jitter(alpha = 0.2) +\n  geom_smooth(aes(group = source), col = \"blue\", method = \"loess\") +\n  geom_hline(yintercept = 1, col = \"red\", lty = 3) + \n  facet_wrap(~source) + \n  theme(axis.text.x = \n          element_text(angle = 90, vjust = 0.5, hjust=1),\n        legend.position = \"NA\") + \n  labs(\n    x = \"Year\",\n    y = \"log(frequency per 1000 words)\"\n  )\n\n\n\n\nNote that the dashed red line is always at the same position (with a value of exp(1) = 2.72). Comparing it with the blue line of best fit for each source for which we have complete data suggests that visually we cannot discern strong trends in the use of condition-first language across the study time period, so using variability-based neighbor clustering (VNC) is unlikely to provide meaningful results for this research question.\nWe can see that the Advertiser seems to have higher median frequencies than others, as does the Northern Territorian. Let’s look at it grouped as tabloid vs broadsheet (with outliers not shown):\n\nCodecondition_first_annotated %>%\n  select(year, source, source_type, frequency) %>%\n  mutate(year = as.factor(year)) %>%\n  group_by(year, source_type) %>%\n  ggplot(aes(x = year, y = frequency, fill = source_type)) +\n  geom_boxplot(outlier.shape = NA) +\n  coord_cartesian(ylim = quantile(condition_first_annotated$frequency, c(0.05, 0.95))) +\n  labs(\n    x = \"\",\n    y = \"Frequency per thousand words\",\n    fill = \"Source type\"\n  )\n\n\n\n\nIt appears that median frequency in tabloids is somewhat higher, although the intervals do overlap across all years.\n\nCodecondition_first_annotated %>%\n  ggplot(aes(x = as.factor(year), \n             y = log(frequency), \n             fill = year)) + \n  geom_jitter(alpha = 0.2) +\n  geom_smooth(aes(group = source_type), col = \"blue\", method = \"lm\") +\n  geom_hline(yintercept = 1, col = \"red\", lty = 3) + \n  facet_wrap(~source_type) + \n  theme(axis.text.x = \n          element_text(angle = 90, vjust = 0.5, hjust=1),\n        legend.position = \"NA\") + \n  labs(\n    x = \"Year\",\n    y = \"log(frequency per 1000 words)\"\n  )\n\n\n\n\nWe can see that the frequency seems to decrease in broadsheets but not in tabloids across years.\nLet’s quickly look at differences by month:\n\nCodecondition_first_annotated_for_modelling %>% \n  select(month_metadata, source, frequency) %>%\n  ggplot(aes(y = frequency, x = month_metadata)) + \n  geom_violin()\n\n\n\n\nThe frequency doesn’t seem to be different month to month, when visualised using a violin or box plots.\n\nCodecondition_first_annotated_for_modelling %>% \n  select(month_metadata, source) %>%\n  group_by(month_metadata, source) %>%\n  mutate(count_source = n()) %>%\n  distinct() %>%\n  ungroup() %>%\n  ggplot(aes(y = count_source, x = month_metadata)) +\n  geom_boxplot()\n\n\n\n\n\nWe will use a linear mixed effects model to consider whether there are differences in the frequency of condition-first language use in broadsheets and tabloids across years, including whether there are differences in specific publications. We will also use simple linear models to explroe\nWhen constructing the model we will:\n\nUse log(frequency) as the dependent variable, as this is normally distributed\nCentre and scale the date\n\n\nCodecondition_first_annotated_for_modelling$scaled_year <- scale(condition_first_annotated_for_modelling$year, scale = F)\nlibrary(broom.mixed)\n# base model\nm_0_base <- glm(log(frequency) ~ 1, family = gaussian, \n                data = condition_first_annotated_for_modelling)\n# with year\nm_0_year <- glm(log(frequency) ~ scaled_year, family = gaussian, \n                data = condition_first_annotated_for_modelling)\n# with year and source type\nm_0_yearsourcetype <- glm(log(frequency) ~ scaled_year + source_type, family = gaussian, \n                data = condition_first_annotated_for_modelling)\n# with year and source type\nm_0_yearsource <- glm(log(frequency) ~ scaled_year + source, family = gaussian, \n                data = condition_first_annotated_for_modelling)\n# with source\nm_0_source = lmer(log(frequency) ~ 1 + (1|source), REML = T, \n                  data = condition_first_annotated_for_modelling)\n\n\nDoes including the source (as a random effect, as we haven’t sampled all of them) improve our model?\n\nCoderbind(\n  {glance(m_0_base) %>% mutate(model = \"Base\") %>% dplyr::select(-df.null, -null.deviance, -deviance)},\n  {glance(m_0_year) %>% mutate(model = \"With year\") %>% dplyr::select(-df.null, -null.deviance, -deviance)},\n  {glance(m_0_yearsourcetype) %>% mutate(model = \"With year & source type\") %>% dplyr::select(-df.null, -null.deviance, -deviance)},\n  {glance(m_0_yearsource) %>% mutate(model = \"With year & source\") %>% dplyr::select(-df.null, -null.deviance, -deviance)},\n  {glance(m_0_source) %>% mutate(model = \"With source\") %>% dplyr::select(-sigma, -REMLcrit)}\n) %>% \n  arrange(AIC)\n\n# A tibble: 5 × 6\n  logLik   AIC   BIC df.residual  nobs model                  \n   <dbl> <dbl> <dbl>       <int> <int> <chr>                  \n1 -3750. 7524. 7596.        3031  3042 With year & source     \n2 -3767. 7542. 7566.        3039  3042 With year & source type\n3 -3775. 7555. 7573.        3039  3042 With source            \n4 -3876. 7759. 7777.        3040  3042 With year              \n5 -3879. 7762. 7774.        3041  3042 Base                   \n\n\nYes, it seems that the AIC and BIC are reduced while the logLik is higher for the model that includes source and year. So, yes, it seems using a random effects model for source may be an option.\nNow let’s build several different random effects models:\n\nIncluding year as a fixed effect\nIncluding each specific source (random effect) individually and year\n\n\nCode#library(afex)\nm_1_base <- lmer(log(frequency) ~ 1 + (1|source), \n                 data = condition_first_annotated_for_modelling,\n                 REML = FALSE, \n                 control = lmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))\n# random intercept for each source\nm_1_year <- lmer(log(frequency) ~ scaled_year + (1|source), \n                 data = condition_first_annotated_for_modelling,\n                 REML = FALSE, \n                 control = lmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))\n# random intercept for each source\nm_1_year_sourcetype <- lmer(log(frequency) ~ scaled_year + source_type +(1|source), \n                 data = condition_first_annotated_for_modelling,\n                 REML = FALSE, \n                 control = lmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))\n# random slope and intercept for each source\nm_1_yearsource <- lmer(log(frequency) ~ scaled_year + (scaled_year|source), \n                 data = condition_first_annotated_for_modelling,\n                 REML = FALSE, \n                 control = lmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))\n# random slope and intercept for each source\nm_1_full <- lmer(log(frequency) ~ scaled_year + source_type + (scaled_year|source), \n                 data = condition_first_annotated_for_modelling,\n                 REML = FALSE, \n                 control = lmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))\n# random intercept for each sourcetype\nm_1_year_sourcetype_nosource <- \n  lmer(log(frequency) ~ scaled_year +(1|source_type), \n                 data = condition_first_annotated_for_modelling,\n                 REML = FALSE, \n                 control = lmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))\n# use the all_fit function to assess which optimisers work\n#all_fit(m_1_yearsource)\n# m_1_yearsource_apex <- \n#   mixed(log(frequency) ~ scaled_year + (scaled_year|source), \n#       data = condition_first_annotated_for_modelling,\n#       method = \"PB\",\n#       REML=FALSE,\n#       control = lmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))\n# m_1_yearsource_apex\n\n\nWe end up needing to use the nlminb optimiser from the optimx library (originally used by lme4), as the default REML fails to converge for the most complex model.\n\nCodepurrr::map_dfr(list(\n  m_1_base,\n  m_1_year,\n  m_1_year_sourcetype,\n  m_1_yearsource,\n  m_1_year_sourcetype_nosource,\n  m_1_full),\n        ~(glance(.x))) %>%\n  mutate(model = c(\n    \"1 + (1|source)\",\n    \"scaled_year + (1|source)\",\n    \"scaled_year + source_type +(1|source)\",\n    \"scaled_year + (scaled_year|source)\",\n    \"scaled_year + (1|sourcetype)\",\n    \"scaled_year + source_type + (scaled_year|source)\"\n  )) %>%\n  arrange(AIC)\n\n# A tibble: 6 × 8\n   nobs sigma logLik   AIC   BIC deviance df.residual model                     \n  <int> <dbl>  <dbl> <dbl> <dbl>    <dbl>       <int> <chr>                     \n1  3042 0.828 -3756. 7525. 7567.    7511.        3035 scaled_year + source_type…\n2  3042 0.832 -3762. 7534. 7564.    7524.        3037 scaled_year + source_type…\n3  3042 0.828 -3764. 7540. 7576.    7528.        3036 scaled_year + (scaled_yea…\n4  3042 0.831 -3772. 7552. 7576.    7544.        3038 scaled_year + (1|source)  \n5  3042 0.832 -3773. 7552. 7570.    7546.        3039 1 + (1|source)            \n6  3042 0.835 -3773. 7553. 7577.    7545.        3038 scaled_year + (1|sourcety…\n\n\nThe full model has the lowest AIC and highest log-Likelihood among the mixed effects models. However, it’s AIC is not that different (7524 vs 7525) to the simpler model scaled_year + source, while the simpler model has a lower BIC and higher logLik.\nLet’s compare the two models: the full mixed effects model and the simple scaled_year + source\n\nCodeanova(m_1_full, m_0_yearsource)\n\nData: condition_first_annotated_for_modelling\nModels:\nm_1_full: log(frequency) ~ scaled_year + source_type + (scaled_year | source)\nm_0_yearsource: log(frequency) ~ scaled_year + source\n               npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)  \nm_1_full          7 7525.1 7567.3 -3755.6   7511.1                       \nm_0_yearsource   12 7523.9 7596.2 -3750.0   7499.9 11.188  5    0.04778 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIt seems that the more complex model does not offer a substantial improvement in fit over the simpler one. Let’s summarise that model.\nCodereport::report(m_0_yearsource)\n‘r2()’ does not support models of class ‘glm’. ‘r2()’ does not support models of class ‘glm’. We fitted a linear model (estimated using ML) to predict frequency with scaled_year and source (formula: log(frequency) ~ scaled_year + source). . The model’s intercept, corresponding to scaled_year = 0 and source = Advertiser, is at 1.28 (95% CI [1.21, 1.36], t(3031) = 32.98, p < .001). Within this model:\n\nThe effect of scaled year is statistically non-significant and negative (beta = -7.05e-03, 95% CI [-0.02, 2.13e-03], t(3031) = -1.50, p = 0.132; Std. beta = -9.82e-03, 95% CI [-0.02, 3.54e-03])\nThe effect of source [Age] is statistically significant and negative (beta = -0.45, 95% CI [-0.57, -0.33], t(3031) = -7.43, p < .001; Std. beta = -0.19, 95% CI [-0.25, -0.14])\nThe effect of source [Australian] is statistically significant and negative (beta = -0.65, 95% CI [-0.79, -0.51], t(3031) = -9.12, p < .001; Std. beta = -0.27, 95% CI [-0.33, -0.21])\nThe effect of source [CanTimes] is statistically significant and negative (beta = -0.37, 95% CI [-0.50, -0.23], t(3031) = -5.33, p < .001; Std. beta = -0.18, 95% CI [-0.24, -0.12])\nThe effect of source [CourierMail] is statistically significant and negative (beta = -0.14, 95% CI [-0.25, -0.03], t(3031) = -2.48, p = 0.013; Std. beta = -0.07, 95% CI [-0.12, -0.02])\nThe effect of source [HeraldSun] is statistically non-significant and negative (beta = -0.06, 95% CI [-0.17, 0.04], t(3031) = -1.18, p = 0.240; Std. beta = -0.05, 95% CI [-0.09, -2.66e-04])\nThe effect of source [HobMercury] is statistically non-significant and positive (beta = 0.13, 95% CI [-0.02, 0.27], t(3031) = 1.68, p = 0.092; Std. beta = 0.05, 95% CI [-0.01, 0.12])\nThe effect of source [NorthernT] is statistically non-significant and positive (beta = 0.16, 95% CI [-0.02, 0.35], t(3031) = 1.72, p = 0.086; Std. beta = 0.04, 95% CI [-0.04, 0.12])\nThe effect of source [SydHerald] is statistically significant and negative (beta = -0.53, 95% CI [-0.64, -0.42], t(3031) = -9.42, p < .001; Std. beta = -0.23, 95% CI [-0.28, -0.19])\nThe effect of source [WestAus] is statistically non-significant and positive (beta = 0.02, 95% CI [-0.12, 0.15], t(3031) = 0.24, p = 0.808; Std. beta = -0.02, 95% CI [-0.08, 0.04])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using\nTo summarise:\n\nThe effect of year was not found to be significant.\nRelative to the Advertiser, the Age, Australian, Canberra Times, Courier Mail and Sydney Morning Herald had less frequency of condition-first language."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Obesity phase 2 project",
    "section": "",
    "text": "This is the landing page for phase 2 of the obesity project."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sydney Informatics Hub",
    "section": "",
    "text": "The Sydney Informatics Hub is a Core Research Facility of the University of Sydney.\nThe use of the SIH services including the Artemis HPC and associated support and training warrants acknowledgement in any publications, conference proceedings or posters describing work facilitated by these services.\nThe continued acknowledgement of the use of SIH facilities ensures the sustainability of our services."
  },
  {
    "objectID": "about.html#suggested-wording",
    "href": "about.html#suggested-wording",
    "title": "Sydney Informatics Hub",
    "section": "Suggested wording",
    "text": "Suggested wording\n\nGeneral acknowledgement:\nThe authors acknowledge the technical assistance provided by the Sydney Informatics Hub, a Core Research Facility of the University of Sydney.\n\n\nAcknowledging specific staff:\nThe authors acknowledge the technical assistance of (name of staff) of the Sydney Informatics Hub, a Core Research Facility of the University of Sydney.\nFor further information about acknowledging the Sydney Informatics Hub, please contact us at sih.info@sydney.edu.au."
  },
  {
    "objectID": "100_data_cleaning_scripts_EDA/02_ExploreSpacyData.html",
    "href": "100_data_cleaning_scripts_EDA/02_ExploreSpacyData.html",
    "title": "Loading and exploring Spacy-annotated tags.",
    "section": "",
    "text": "Codelibrary(here)\n\nhere() starts at /Users/darya/Dropbox (Personal)/Mac/Documents/Projects/PIPE-3034-obesity2\n\nCodelibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n    chisq.test, fisher.test\n\nCodelibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n    intersect, setdiff, setequal, union\n\nCodelibrary(ggplot2)\nlibrary(tidyr)\ntheme_set(theme_minimal())\n\n\n\nCoderead_spacy <- function(filename){\n  read_csv(\n    here(\"../pipe-1951-obesity/300_data_processed\", filename)) %>%\n    # drop the row number column\n    select(-`...1`)\n}\nspacy_output_body <- read_spacy(\"pos_bodies_annotated_with_spacy.csv\") %>% mutate(source = \"body\") %>% distinct()\n\nNew names:\nRows: 59429 Columns: 9\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(8): article_id, sentence, text, tag, dep, head, left, right dbl (1): ...1\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -> `...1`\n\nCodespacy_output_title <- read_spacy(\"pos_titles_annotated_with_spacy.csv\") %>% mutate(source = \"title\") %>% distinct()\n\nNew names:\nRows: 1786 Columns: 9\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(8): article_id, sentence, text, tag, dep, head, left, right dbl (1): ...1\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -> `...1`\n\nCodespacy_output <- rbind(spacy_output_body, spacy_output_title) %>% arrange(article_id)\n#\nread_cqpweb <- function(filename){\n  read.csv(\n    here(\"100_data_raw\", filename), \n    skip = 3, sep = \"\\t\") %>% \n    janitor::clean_names()\n}\ncondition_first <- read_cqpweb(\"aoc_all_condition_first.txt\")\nperson_first <- read_cqpweb(\"aoc_all_person_first.txt\")\nadj_obese <- read_cqpweb(\"aoc_all_obese_tagadjlemma.txt\")\nadj_overweight <- read_cqpweb(\"aoc_all_overweight_tagadjlemma.txt\")\nmetadata <- read_csv(here(\"100_data_raw\", \"corpus_cqpweb_metadata.csv\"))\n\nRows: 26163 Columns: 36\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (10): article_id, shortcode, month_metadata, rownumber, metadata, title...\ndbl  (23): year, yearmo, wordcount_from_metatata, wordcount_body, wordcount_...\nlgl   (2): obesity_boolean_ukcorpus, obesity_boolean_header\ndate  (1): date\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nCodecondition_first_annotated <- inner_join(\n  metadata, condition_first, by = c(\"article_id\" = \"text\" ))\nperson_first_annotated <- inner_join(\n  metadata, person_first, by = c(\"article_id\" = \"text\" ))\nmonika_condition_first <- c(\"adolescent\", \"adolescents\", \"adult\", \"adults\", \"American\", \"Americans\", \"amputee\", \"amputees\", \"asthmatic\", \"asthmatics\", \"aussie\", \"aussies\", \"australian\", \"Australians\", \"banker\", \"bankers\", \"boss\", \"bosses\", \"boy\", \"boys\", \"Brit\", \"Brits\", \"Canberran\", \"Canberrans\", \"child\", \"children\", \"citizen\", \"citizens\", \"client\", \"clients\", \"contestant\", \"contestants\", \"customer\", \"customers\", \"dad\", \"dads\", \"daughter\", \"daughters\", \"diabetic\", \"diabetics\", \"dieter\", \"dieters\", \"driver\", \"drivers\", \"employee\", \"employees\", \"fan\", \"fans\", \"father\", \"fathers\", \"Frenchman\", \"Frenchmen\", \"Frenchwomen\", \"friend\", \"friends\", \"girl\", \"girls\", \"guy\", \"guys\", \"individual\", \"individuals\", \"kid\", \"kids\", \"ladies\", \"lady\", \"man\", \"men\", \"model\", \"models\", \"mother\", \"mothers\", \"motorist\", \"motorists\", \"mum\", \"mums\", \"pal\", \"pals\", \"parent\", \"parents\", \"participant\", \"participants\", \"passenger\", \"passengers\", \"patient\", \"patients\", \"people\", \"person\", \"persons\", \"preschooler\", \"preschoolers\", \"Queenslander\", \"Queenslanders\", \"resident\", \"residents\", \"smoker\", \"smokers\", \"socialite\", \"socialites\", \"soldier\", \"soldiers\", \"son\", \"sons\", \"student\", \"students\", \"subject\", \"subjects\", \"Tasmanian\", \"Tasmanians\", \"teen\", \"teenager\", \"teenagers\", \"teens\", \"traveller\", \"travellers\", \"Victorian\", \"Victorians\", \"volunteer\", \"volunteers\", \"woman\", \"women\", \"worker\", \"workers\", \"youngster\", \"youngsters\")\n\n\nMonika defined condition-first language in the following way:\n“obese (adolescent|adolescents|adult|adults|American|Americans|amputee|amputees|asthmatic|asthmatics|aussie|aussies|australian|Australians|banker|bankers|boss|bosses|boy|boys|Brit|Brits|Canberran|Canberrans|child|children|citizen|citizens|client|clients|contestant|contestants|customer|customers|dad|dads|daughter|daughters|diabetic|diabetics|dieter|dieters|driver|drivers|employee|employees|fan|fans|father|fathers|Frenchman|Frenchmen|Frenchwomen|friend|friends|girl|girls|guy|guys|individual|individuals|kid|kids|ladies|lady|man|men|model|models|mother|mothers|motorist|motorists|mum|mums|pal|pals|parent|parents|participant|participants|passenger|passengers|patient|patients|people|person|persons|preschooler|preschoolers|Queenslander|Queenslanders|resident|residents|smoker|smokers|socialite|socialites|soldier|soldiers|son|sons|student|students|subject|subjects|Tasmanian|Tasmanians|teen|teenager|teenagers|teens|traveller|travellers|Victorian|Victorians|volunteer|volunteers|woman|women|worker|workers|youngster|youngsters)”.\nAnd person-first in the following way:\nadolescent|adolescents|adult|adults|American|Americans|amputee|amputees|asthmatic|asthmatics|aussie|aussies|australian|Australians|banker|bankers|boss|bosses|boy|boys|Brit|Brits|Canberran|Canberrans|child|children|citizen|citizens|client|clients|contestant|contestants|customer|customers|dad|dads|daughter|daughters|diabetic|diabetics|dieter|dieters|driver|drivers|employee|employees|fan|fans|father|fathers|Frenchman|Frenchmen|Frenchwomen|friend|friends|girl|girls|guy|guys|individual|individuals|kid|kids|ladies|lady|man|men|model|models|mother|mothers|motorist|motorists|mum|mums|pal|pals|parent|parents|participant|participants|passenger|passengers|patient|patients|people|person|persons|preschooler|preschoolers|Queenslander|Queenslanders|resident|residents|smoker|smokers|socialite|socialites|soldier|soldiers|son|sons|student|students|subject|subjects|Tasmanian|Tasmanians|teen|teenager|teenagers|teens|traveller|travellers|Victorian|Victorians|volunteer|volunteers|woman|women|worker|workers|youngster|youngsters|those|many) with * obesity\n\nCodecondition_first_spacy_monika_defined <- \n  spacy_output %>%\n  filter(text == \"obese\") %>%\n  filter(dep != \"conj\" & dep != \"attr\") %>%\n  filter(head %in% monika_condition_first)\ncondition_first_spacy_monika_filtered_out <- \n  spacy_output %>%\n  filter(text == \"obese\") %>%\n  filter(dep != \"conj\" & dep != \"attr\") %>%\n  filter(!(article_id %in% condition_first_spacy_monika_defined$article_id))\n\n\nLet’s join and compare the CQP web output for condition-first language with the Spacy output, if we use Monika’s predefined list:\n\nCodecondition_first_full <- full_join(\n  condition_first_annotated,\n  {condition_first_spacy_monika_defined %>% group_by(article_id) %>% count()}\n)\n\nJoining, by = \"article_id\""
  },
  {
    "objectID": "100_data_cleaning_scripts_EDA/01_ExploreCQPwebData.html",
    "href": "100_data_cleaning_scripts_EDA/01_ExploreCQPwebData.html",
    "title": "Loading and exploring the data",
    "section": "",
    "text": "In the below document, we show that we should use the Python-generated total word count for calculating normalised frequencies (or the counts provided in the metadata, the two are nearly identical and well correlated across all text lengths.\n\nIn this file, we start by loading and exploring the data for the obesity corpus.\n\nCodelibrary(here)\nlibrary(janitor)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(knitr)\ntheme_set(theme_minimal())\nread_cqpweb <- function(filename){\n  read.csv(\n    here(\"100_data_raw\", filename), \n    skip = 3, sep = \"\\t\") %>% \n    janitor::clean_names()\n}\nclean_fatneglabel <- function(dirname){\n  purrr::map_dfr(\n    list.files(\n      here(\"100_data_raw\", dirname )\n      ),\n    ~{read.csv(\n      here(\n        paste0(\"100_data_raw/\", dirname), .x), \n      skip = 3, sep = \"\\t\") %>% \n        janitor::clean_names()\n      })\n}\ncondition_first <- read_cqpweb(\"aoc_all_condition_first.txt\")\nperson_first <- read_cqpweb(\"aoc_all_person_first.txt\")\nadj_obese <- read_cqpweb(\"aoc_all_obese_tagadjlemma.txt\")\nadj_overweight <- read_cqpweb(\"aoc_all_overweight_tagadjlemma.txt\")\nfat_labelled <- clean_fatneglabel(\"fat_neg_label_yes_textFreqs\")\nmetadata <- read_csv(here(\"100_data_raw\", \"corpus_cqpweb_metadata.csv\"))\nadditional_source_metadata <- read_csv(here(\"100_data_raw\", \"addition_source_metadata.csv\"))\nmetadata_full <- inner_join(metadata, additional_source_metadata)\ncondition_first_annotated <- inner_join(\n  condition_first, metadata_full, by = c(\"text\" = \"article_id\"))\nperson_first_annotated <- inner_join(\n  person_first, metadata_full, by = c(\"text\" = \"article_id\"))\nfat_annotated  <- inner_join(\n  fat_labelled, metadata_full, by = c(\"text\" = \"article_id\")\n)\nsaveRDS(fat_annotated, file = here::here(\"200_data_clean\", \"fat_annotated\"))\n\n\nCheck that all text in the CQP web export file are found in the article_id of the metadata file.\n\nCodejanitor::tabyl(condition_first$text %in% metadata$article_id) %>% kable()\n\n\n\ncondition_first\\(text %in% metadata\\)article_id\nn\npercent\n\n\nTRUE\n3208\n1\n\n\n\nCodejanitor::tabyl(person_first$text %in% metadata$article_id) %>% kable()\n\n\n\nperson_first\\(text %in% metadata\\)article_id\nn\npercent\n\n\nTRUE\n106\n1\n\n\n\nCodejanitor::tabyl(adj_obese$text %in% metadata$article_id) %>% kable()\n\n\n\nadj_obese\\(text %in% metadata\\)article_id\nn\npercent\n\n\nTRUE\n10821\n1\n\n\n\nCodejanitor::tabyl(adj_overweight$text %in% metadata$article_id) %>% kable()\n\n\n\nadj_overweight\\(text %in% metadata\\)article_id\nn\npercent\n\n\nTRUE\n7136\n1\n\n\n\n\nYes, this is true for all of them."
  },
  {
    "objectID": "100_data_cleaning_scripts_EDA/01_ExploreCQPwebData.html#articles-by-month-year-and-publication-based-on-metadata",
    "href": "100_data_cleaning_scripts_EDA/01_ExploreCQPwebData.html#articles-by-month-year-and-publication-based-on-metadata",
    "title": "Loading and exploring the data",
    "section": "Articles by month, year and publication (based on metadata)",
    "text": "Articles by month, year and publication (based on metadata)\nHave we sampled the articles consistently by month, year and publication?\n\nCodeassess_year_source <- function(df, var){\n  var <- enquo(var)\n  df %>% \n  select(source, !!var) %>%\n  group_by(source, !!var) %>% \n  count(!!var) %>%\n  rename(count = n) %>%\n  pivot_wider(id_cols = c(source), names_from = !!var, values_from = c(count), values_fill = 0) %>%\n  janitor::adorn_totals(c(\"row\", \"col\")) %>%\n  kable()\n} \nassess_year_source(metadata_full, year)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsource\n2008\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\nTotal\n\n\n\nAdvertiser\n386\n309\n300\n284\n326\n395\n287\n293\n221\n210\n164\n174\n3349\n\n\nAge\n354\n243\n218\n223\n204\n317\n220\n325\n234\n191\n177\n120\n2826\n\n\nAustralian\n296\n221\n178\n201\n168\n172\n155\n110\n95\n113\n147\n104\n1960\n\n\nBrisTimes\n0\n0\n0\n0\n0\n5\n11\n81\n16\n14\n53\n48\n228\n\n\nCanTimes\n188\n188\n166\n180\n165\n238\n218\n212\n114\n154\n149\n72\n2044\n\n\nCourierMail\n382\n334\n309\n274\n239\n296\n246\n226\n188\n200\n244\n193\n3131\n\n\nHeraldSun\n459\n409\n404\n379\n338\n413\n273\n252\n196\n209\n214\n176\n3722\n\n\nHobMercury\n199\n141\n149\n98\n135\n177\n88\n85\n107\n91\n101\n94\n1465\n\n\nNorthernT\n104\n94\n97\n67\n64\n73\n56\n61\n64\n59\n43\n40\n822\n\n\nSydHerald\n349\n311\n352\n329\n338\n342\n265\n372\n285\n237\n242\n214\n3636\n\n\nTelegraph\n0\n0\n0\n0\n0\n2\n190\n134\n190\n233\n191\n149\n1089\n\n\nWestAus\n283\n222\n221\n210\n185\n190\n210\n114\n119\n80\n40\n17\n1891\n\n\nTotal\n3000\n2472\n2394\n2245\n2162\n2620\n2219\n2265\n1829\n1791\n1765\n1401\n26163\n\n\n\n\n\nWe can see that we are: - missing articles from the Brisbane Times and Daily Telegraph in 2008-2012. - There are fewer articles in 2019 than in previous years. Let’s explore if we have data for all of the months for that year:\n\nCodemetadata_full %>%\n  filter(year == 2019) %>%\n  assess_year_source(., month_metadata)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsource\n01\n02\n03\n04\n05\n06\n07\n08\n09\n10\n11\n12\nTotal\n\n\n\nAdvertiser\n19\n14\n16\n23\n21\n16\n10\n14\n12\n11\n9\n9\n174\n\n\nAge\n9\n6\n4\n0\n12\n12\n9\n10\n13\n15\n16\n14\n120\n\n\nAustralian\n8\n11\n11\n6\n11\n7\n10\n14\n3\n8\n9\n6\n104\n\n\nBrisTimes\n4\n2\n2\n6\n3\n5\n6\n3\n6\n5\n3\n3\n48\n\n\nCanTimes\n11\n6\n10\n10\n9\n0\n5\n5\n5\n6\n4\n1\n72\n\n\nCourierMail\n15\n24\n17\n24\n0\n16\n18\n18\n17\n17\n8\n19\n193\n\n\nHeraldSun\n16\n23\n10\n20\n15\n17\n14\n16\n12\n14\n6\n13\n176\n\n\nHobMercury\n7\n17\n15\n10\n1\n8\n7\n8\n4\n3\n9\n5\n94\n\n\nNorthernT\n3\n7\n3\n2\n3\n2\n5\n3\n1\n6\n1\n4\n40\n\n\nSydHerald\n15\n14\n18\n14\n22\n19\n17\n17\n19\n22\n21\n16\n214\n\n\nTelegraph\n13\n20\n9\n23\n10\n18\n13\n12\n7\n10\n8\n6\n149\n\n\nWestAus\n1\n0\n3\n1\n1\n2\n0\n2\n2\n3\n0\n2\n17\n\n\nTotal\n121\n144\n118\n139\n108\n122\n114\n122\n101\n120\n94\n98\n1401\n\n\n\n\n\nWe can see that we do have data for each month from 2019, although there are many fewer articles that year from the Western Australian and to a lesser extent the Canberra Times.\nLet’s also visualise this trend by publication\n\nCodemetadata_full %>%\n  group_by(source, year) %>%\n  count() %>%\n  ggplot(aes(col = source, y = n, x = year)) + \n  geom_line() +\n  scale_x_continuous(breaks = unique(metadata_full$year)) + \n  labs(x = \"\", y = \"Number of articles\")\n\n\n\n\nWe can also look at this using facets:\n\nCodemetadata_full %>%\n  group_by(source, year) %>%\n  count() %>%\n  ggplot(aes(col = source, y = n, x = year)) + \n  geom_line() +\n  scale_x_continuous(breaks = unique(metadata_full$year)) + \n  labs(x = \"\", y = \"Number of articles\") + \n  facet_wrap(~source, scales = \"free_y\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),\n        legend.position = \"NA\")\n\n\n\n\nThis most clearly shows that the number of articles mentioning obes* declines with time in Australian media.\nThis could be due to:\n\nFewer articles in general being published\nProportionally fewer articles about obesity being written\nMore duplicate articles not having been cleaned in the earlier years\n\nIt is important for us to acknowledge this trend, as it will influence our inferences around using the raw number of articles in temporal comparisons.\n\nCodemetadata_full %>%\n  # don't have all years of data for these\n  filter(!(source %in% c(\"BrisTimes\", \"Telegraph\"))) %>%\n  group_by(source, year) %>%\n  count() %>%\n  ggplot(aes(x = year, y = n, group = year)) + \n  geom_boxplot() +\n  scale_x_continuous(breaks = unique(metadata_full$year)) + \n  labs(x = \"\", y = \"Number of articles\")"
  },
  {
    "objectID": "100_data_cleaning_scripts_EDA/01_ExploreCQPwebData.html#cqp-web-vs-metadata-annotated-and-python-quantitated-word-counts",
    "href": "100_data_cleaning_scripts_EDA/01_ExploreCQPwebData.html#cqp-web-vs-metadata-annotated-and-python-quantitated-word-counts",
    "title": "Loading and exploring the data",
    "section": "CQP-web vs metadata-annotated and Python-quantitated word counts",
    "text": "CQP-web vs metadata-annotated and Python-quantitated word counts\nWhen carrying out modelling, it is often important to use data normalised to article word counts. This is dependent upon getting a correct word count for each article. Below, we compare article word counts from CQPWeb to those generated by Python and reported in the metadata in Lexis, for the condition-first language dataset as an example. The effects seen here will be consisten across other datasets as well.\nFirst, compare the counts from Python and the metadata:\n\nCodep <- condition_first_annotated %>%\n  ggplot(aes(x = wordcount_total,\n             y = wordcount_from_metatata)) + \n  geom_point() + \n  geom_smooth(method = \"loess\", formula = y ~ x, col = \"red\") + \n  geom_abline(slope = 1, intercept = 0, col = \"blue\", lty = 2) +\n  xlab(\"Word count, Python\") + \n  ylab(\"Word count, metadata\") \nplotly::ggplotly(p) \n\n\n\n\n\nThe word counts are well correlated across all text lengths.\nNext, we compare the word counts from CQP-web and the metadata:\n\nCodep <- condition_first_annotated %>%\n  ggplot(aes(x = no_words_in_text,\n             y = wordcount_from_metatata)) + \n  geom_point() + \n  geom_abline(slope = 1, intercept = 0, col = \"blue\", lty = 2) +\n  xlab(\"Word count, CQPWeb\") + \n  ylab(\"Word count, metadata\") \nplotly::ggplotly(p) \n\n\n\n\n\nWe can see that the longer the text, the more the counts do not match what is in the metadata (or counted via Python). This is most likely to CQP-Web counting punctuation as tokens, thereby with longer text more punctuation is added to each text.\nWe can add a smoothed conditional means line (in red) to show the deviation between the identity line (y = x, shown dashed in blue) and the line of best fit:\n\nCodecondition_first_annotated %>%\n  ggplot(aes(x = no_words_in_text,\n             y = wordcount_from_metatata)) + \n  geom_point() + \n  geom_smooth(method = \"loess\", formula = y ~ x, se = TRUE, col = \"red\") + \n  geom_abline(slope = 1, intercept = 0, col = \"blue\", lty = 2) +\n  xlab(\"Word count, CQPWeb\") + \n  ylab(\"Word count, metadata\") \n\n\n\n\nTo make this more apparent, we plot the difference between the metadata-provided and Python and CQP-web counts:\n\nCodecondition_first_annotated %>%\n  mutate(\n    `Python - metadata` = (abs(wordcount_total - wordcount_from_metatata)),\n    `CQPweb - metadata` = (abs(no_words_in_text - wordcount_from_metatata)),\n    word_count_quartile = ntile(wordcount_from_metatata, 10)) %>%\n  select(`Python - metadata`, `CQPweb - metadata`, word_count_quartile) %>%\n  pivot_longer(cols = c(`Python - metadata`, `CQPweb - metadata`)) %>%\n  ggplot(aes(x = as.factor(word_count_quartile), \n             y = value,\n             fill = name)) + geom_boxplot() +\n  labs(\n    x = \"Text length, decile, based on metadata supplied word count\",\n    y = \"Absolute difference in word count between X and metadata-provided counts\",\n   caption = \"The longer the text, the more the CQP-Web count diverges from that of the metadata (and Python)\") + guides(fill=guide_legend(title=\"Difference\"))\n\n\n\n\nThis also affects the normalised counts. To show this, we have broken the dataset into 6 groups based on text length, with 1 being the shortest and 6 being the longest texts. We can see that for the ~500 longest texts (panel 6), the line of best fit between frequencies is curved, showcasing how the normalised frequency is under-estimated for longer texts (due to the denominator for normalisation being bigger, as punctuation is included in the calculation).\n\nCodecondition_first_annotated %>%\n  mutate(frequency = 10^6*no_hits_in_text/wordcount_total,\n         length_quantile = ntile(desc(condition_first_annotated$wordcount_total),6)) %>%\n  ggplot(aes(x = frequency, y = freq_per_million_words)) + \n  geom_point() + \n  geom_smooth(method = \"loess\", formula = y ~ x, se = TRUE, col = \"red\") + \n  geom_abline(slope = 1, intercept = 0, col = \"blue\", lty = 2) +\n  facet_wrap(~length_quantile, scales = \"free_y\") +\n  labs(x = \"Python-counted frequency per million words\",\n       y = \"CQP-calculated frequency per million words\")\n\n\n\n\nCONCLUSION: Use the Python-generated total word count for calculating normalised frequencies (or the counts provided in the metadata, the two are nearly identical and well correlated across all text lengths. This also means we cannot use the provided normalised frequencies from CQPweb."
  },
  {
    "objectID": "100_data_cleaning_scripts_EDA/01_ExploreCQPwebData.html#combining-fat-negative-label-files",
    "href": "100_data_cleaning_scripts_EDA/01_ExploreCQPwebData.html#combining-fat-negative-label-files",
    "title": "Loading and exploring the data",
    "section": "Combining fat negative label files",
    "text": "Combining fat negative label files"
  },
  {
    "objectID": "000_scoping/scoping.html",
    "href": "000_scoping/scoping.html",
    "title": "PIPE-3034: Obesity stigma analysis with pattern frequency over time and publication",
    "section": "",
    "text": "Scope link, Sharepoint Usyd"
  },
  {
    "objectID": "000_scoping/scoping.html#project-details",
    "href": "000_scoping/scoping.html#project-details",
    "title": "PIPE-3034: Obesity stigma analysis with pattern frequency over time and publication",
    "section": "1. Project details",
    "text": "1. Project details\n\nClients: Monika Bednarek\nFaculty: FASS\nCollaborators and their affiliations: Carly Bray (USyd PhD Student), Tara (Lancaster),"
  },
  {
    "objectID": "000_scoping/scoping.html#project-scoping-details",
    "href": "000_scoping/scoping.html#project-scoping-details",
    "title": "PIPE-3034: Obesity stigma analysis with pattern frequency over time and publication",
    "section": "Project scoping details",
    "text": "Project scoping details\n\nScope prepared by Joel Nothman, Feb 2022\nProject manager - Marius Mather"
  },
  {
    "objectID": "000_scoping/scoping.html#project-summary",
    "href": "000_scoping/scoping.html#project-summary",
    "title": "PIPE-3034: Obesity stigma analysis with pattern frequency over time and publication",
    "section": "2. Project summary",
    "text": "2. Project summary\n\nResearch context\n\nIn PIPE-1951, SIH supported Monika to prepare a corpus for studying mentions of obesity in Australian media. As a collaboration with the Obesity Collective, the project hopes to understand the changing nature of reference to obesity over time and publication, with a focus on stigma.\n\n\n\nClient needs\n\nThis project will create one or more notebooks for inclusion in the Australian Text Analytics Platform (ATAP) library. The notebooks will support the analysis of language trends for this specific project, with components that are reusable for other research, and guidance in the notebook on how the components might be reapplied.\nThe raw language data will not be available in full to the notebooks. Rather, Monika and her RAs will investigate patterns of interest and construct cross-tabulations of their frequency by publication year and source publication. This project will implement visualisation of comparisons across publications and trends over time, and tools for estimating trends and other significant variation.\nAnalysis may also take into account typical political and organisation affiliation of news sources.\nWhile this project will primarily deal with already quantified phenomena, the linguistic context will have bearing in relation to:\n\nThe choice of statistic, e.g. absolute or relative frequencies; term frequencies vs document frequencies.\nThe comparison of statistics when linguistic features are considered as “alternatives” or part of “confusion sets” (e.g. comparing the frequency of stigmatising and non-stigmatising variants).\nThese statistics will represent the number of mentions of a phenomenon across the entire corpus. For example, counts of “lazy” mentioned in the context of “obes*” will be provided for each (source, year). Outside of the notebook, close analysis will have been applied to estimate, on a sample, the proportion of “lazy” references that are stigmatising of obesity. Applying the assumption that this proportion is invariant with respect to source and year, we will want to estimate the overall quantity of stigmatizing uses.\nThe interpretation of the results and their impact on further analysis.\n\n\n\n\nCurrent data\n\n26,163 articles (16M tokens) in text format with metadata in spreadsheet, acquired from LexisNexis. Covers 2008 to 2019 in 10 publications and 2014 to 2019 in 2 publications"
  },
  {
    "objectID": "000_scoping/scoping.html#project-implementation",
    "href": "000_scoping/scoping.html#project-implementation",
    "title": "PIPE-3034: Obesity stigma analysis with pattern frequency over time and publication",
    "section": "3. Project Implementation",
    "text": "3. Project Implementation\n\nProject plan\n\n[negligible time] Create or use a repository configured with continuous notebook compilation (cf. ATAP notebook repo,, currently private) \n[1 weeks] Implement initial code to visualise and summarise key features of a cross-tabulation \n[1.5 weeks] Research and implement variant analyses for \n\nterm and document frequency \nrelative and absolute frequency \ncomparative frequency within confusion sets \n\n[1.5 week] Iterative improvement to the above; Support the researcher to interpret results \n[1 weeks] Prepare notebook for publication and reuse \n\nReport on key findings. \nDescribe usage in other contexts, within the notebook. \nInclude demonstration of computing a contingency table from a DataFrame of texts and metadata. \n\n\n\n\nResources required\n\nStatistics, analytics, visualisation\nFamiliarity with text analytics\n\n\n\nScheduling\n\nClient and RA will be available for fortnightly meetings.\n\n\n\nDeliverables\n\nAnalysis\nReusable notebooks\n\n\n\nExit conditions\n\nWork will be completed to budget.\n\n\n\nOut of scope\n\nWork beyond time scoped without rebudgeting.\nWriting to paper.\n\n\n\nCommunication plan\n\nFortnightly meetings, ad-hoc communication by email.\nData is on RDS.\n\n\n\nMaintenance plan\n\nNotebooks delivered on GitHub.com with dependencies specified, lodged with ATAP.\n\nTotal budget: 4 weeks FTE effort towards LDaCA RDC"
  }
]